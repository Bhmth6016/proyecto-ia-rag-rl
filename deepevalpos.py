#!/usr/bin/env python3
import os
import sys
import json
import time
import argparse
import logging
import random
import math
import pickle
import torch
import numpy as np
from typing import List, Set, Dict, Any, Tuple, Optional
from pathlib import Path
from collections import defaultdict
from datetime import datetime

# --- Configuraci√≥n logger ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# --- CARGAR MODELOS RL ENTRENADOS ---
MODELS_DIR = Path(r"C:\Users\evill\OneDrive\Documentos\Github\github\proyecto-ia-rag-rl\models\rl_models")

class RLModelManager:
    """Gestor de modelos RL entrenados"""
    
    def __init__(self, models_dir: Path = MODELS_DIR):
        self.models_dir = models_dir
        self.models = {}
        self._load_trained_models()
    
    def _load_trained_models(self):
        """Carga todos los modelos RL entrenados disponibles"""
        if not self.models_dir.exists():
            logger.warning(f"‚ö†Ô∏è Directorio de modelos no encontrado: {self.models_dir}")
            return
        
        logger.info(f"üîç Buscando modelos RL entrenados en: {self.models_dir}")
        
        # Cargar modelos RLHF
        for model_file in self.models_dir.glob("*.pkl"):
            try:
                model_name = model_file.stem
                with open(model_file, 'rb') as f:
                    model_data = pickle.load(f)
                
                if isinstance(model_data, dict):
                    self.models[model_name] = model_data
                    logger.info(f"‚úÖ Modelo RL cargado: {model_name} (tipo: {model_data.get('type', 'unknown')})")
                else:
                    logger.warning(f"Formato de modelo no reconocido en {model_file}")
                    
            except Exception as e:
                logger.error(f"Error cargando {model_file}: {e}")
        
        # Cargar modelos PyTorch
        for model_file in self.models_dir.glob("*.pt"):
            try:
                model_name = model_file.stem
                model = torch.load(model_file, map_location='cpu')
                self.models[model_name] = {
                    'model': model,
                    'type': 'pytorch',
                    'loaded_at': datetime.now().isoformat()
                }
                logger.info(f"‚úÖ Modelo PyTorch cargado: {model_name}")
            except Exception as e:
                logger.error(f"Error cargando PyTorch model {model_file}: {e}")
        
        # Cargar configuraciones
        config_file = self.models_dir / "training_config.json"
        if config_file.exists():
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    self.config = json.load(f)
                logger.info(f"‚úÖ Configuraci√≥n de entrenamiento cargada")
            except Exception as e:
                logger.error(f"Error cargando configuraci√≥n: {e}")
        
        logger.info(f"üìä Total modelos cargados: {len(self.models)}")
    
    def get_model(self, model_name: str) -> Optional[Dict]:
        """Obtiene un modelo espec√≠fico por nombre"""
        return self.models.get(model_name)
    
    def apply_rl_scoring(self, query: str, product_ids: List[str], context: Dict = None) -> Dict[str, float]:
        """Aplica scoring RL a productos basado en el modelo entrenado"""
        if not self.models:
            return {}
        
        scores = {}
        
        # Intentar usar modelo RLHF si est√° disponible
        rlhf_model = self.models.get('rlhf_reward_model')
        if rlhf_model and rlhf_model.get('type') == 'rlhf':
            try:
                # Extraer caracter√≠sticas del query
                query_features = self._extract_query_features(query)
                
                # Calcular scores basados en el modelo entrenado
                for pid in product_ids:
                    # Simular scoring RL (en producci√≥n esto usar√≠a el modelo real)
                    base_score = random.uniform(0.3, 0.7)
                    
                    # Ajustar basado en similitud con queries de entrenamiento
                    if 'training_queries' in rlhf_model:
                        training_queries = rlhf_model['training_queries']
                        similarity = self._calculate_query_similarity(query, training_queries)
                        base_score += similarity * 0.3
                    
                    # Ajustar basado en feedback hist√≥rico
                    if 'feedback_history' in rlhf_model:
                        feedback = rlhf_model['feedback_history'].get(pid, {})
                        if feedback.get('positive', 0) > 0:
                            base_score += 0.2
                        if feedback.get('negative', 0) > 0:
                            base_score -= 0.1
                    
                    scores[pid] = max(0.0, min(1.0, base_score))
                
                logger.debug(f"üîç RL scoring aplicado a {len(scores)} productos")
                
            except Exception as e:
                logger.error(f"Error aplicando RL scoring: {e}")
        
        return scores
    
    def _extract_query_features(self, query: str) -> Dict:
        """Extrae caracter√≠sticas del query para RL"""
        words = query.lower().split()
        return {
            'length': len(words),
            'contains_question': any(q in query.lower() for q in ['?', 'qu√©', 'c√≥mo', 'd√≥nde']),
            'contains_platform': any(p in query.lower() for p in ['nintendo', 'playstation', 'xbox', 'switch']),
            'contains_genre': any(g in query.lower() for g in ['aventura', 'acci√≥n', 'deportes', 'estrategia']),
            'contains_price': any(p in query.lower() for p in ['barato', 'econ√≥mico', 'precio', 'oferta'])
        }
    
    def _calculate_query_similarity(self, query: str, training_queries: List[str]) -> float:
        """Calcula similitud con queries de entrenamiento"""
        if not training_queries:
            return 0.0
        
        query_lower = query.lower()
        best_similarity = 0.0
        
        for training_query in training_queries[:10]:  # Limitar para rendimiento
            training_lower = training_query.lower()
            
            # Similitud simple por palabras en com√∫n
            query_words = set(query_lower.split())
            training_words = set(training_lower.split())
            
            if query_words and training_words:
                common = query_words.intersection(training_words)
                similarity = len(common) / max(len(query_words), len(training_words))
                best_similarity = max(best_similarity, similarity)
        
        return best_similarity
    
    def get_training_stats(self) -> Dict:
        """Obtiene estad√≠sticas de entrenamiento"""
        stats = {
            'total_models': len(self.models),
            'model_types': {},
            'training_info': {}
        }
        
        for name, model_data in self.models.items():
            model_type = model_data.get('type', 'unknown')
            stats['model_types'][model_type] = stats['model_types'].get(model_type, 0) + 1
            
            # Extraer info de entrenamiento si est√° disponible
            if 'training_stats' in model_data:
                stats['training_info'][name] = model_data['training_stats']
        
        # A√±adir configuraci√≥n general si existe
        if hasattr(self, 'config'):
            stats['global_config'] = self.config
        
        return stats

# Inicializar gestor de modelos RL
try:
    rl_manager = RLModelManager()
    RL_MODELS_AVAILABLE = len(rl_manager.models) > 0
    logger.info(f"ü§ñ Modelos RL disponibles: {RL_MODELS_AVAILABLE} ({len(rl_manager.models)} modelos)")
except Exception as e:
    logger.error(f"Error inicializando RLModelManager: {e}")
    rl_manager = None
    RL_MODELS_AVAILABLE = False

# --- DATOS DE PRUEBA MEJORADOS CON ML ---
TEST_PRODUCTS = [
    # PRODUCTOS DE NINTENDO CON EMBEDDINGS ML
    {"id": "N001", "title": "Nintendo Switch OLED", "category": "consolas", 
     "popularity": 0.95, "price": 349.99, "brand": "Nintendo", 
     "features": ["portable", "oled", "hybrid", "mario", "zelda", "nintendo", "switch"],
     "embedding": [0.8, 0.1, 0.3, 0.9, 0.2, 0.1, 0.4, 0.6],  # Embedding simulado
     "ml_processed": True},
    
    {"id": "N002", "title": "Super Mario Odyssey", "category": "videojuegos", 
     "popularity": 0.9, "price": 59.99, "brand": "Nintendo",
     "features": ["mario", "platformer", "adventure", "switch", "nintendo"],
     "embedding": [0.9, 0.2, 0.1, 0.8, 0.3, 0.1, 0.5, 0.7],
     "ml_processed": True},
    
    {"id": "N003", "title": "Zelda: Breath of the Wild", "category": "videojuegos", 
     "popularity": 0.92, "price": 59.99, "brand": "Nintendo",
     "features": ["zelda", "open-world", "adventure", "switch", "nintendo"],
     "embedding": [0.7, 0.3, 0.4, 0.6, 0.8, 0.2, 0.3, 0.9],
     "ml_processed": True},
    
    {"id": "N004", "title": "Mario Kart 8 Deluxe", "category": "videojuegos", 
     "popularity": 0.88, "price": 59.99, "brand": "Nintendo",
     "features": ["mario", "kart", "racing", "multiplayer", "switch"],
     "embedding": [0.6, 0.4, 0.5, 0.7, 0.6, 0.3, 0.4, 0.8],
     "ml_processed": True},
    
    {"id": "N005", "title": "Animal Crossing: New Horizons", "category": "videojuegos", 
     "popularity": 0.85, "price": 59.99, "brand": "Nintendo",
     "features": ["simulation", "life", "island", "multiplayer", "switch"],
     "embedding": [0.5, 0.5, 0.6, 0.5, 0.7, 0.4, 0.2, 0.7],
     "ml_processed": True},
    
    # PRODUCTOS DE MARIO CON EMBEDDINGS ML
    {"id": "M001", "title": "Super Mario 3D All-Stars", "category": "videojuegos", 
     "popularity": 0.8, "price": 59.99, "brand": "Nintendo",
     "features": ["mario", "collection", "3d", "64", "nintendo"],
     "embedding": [0.8, 0.2, 0.2, 0.8, 0.4, 0.2, 0.3, 0.6],
     "ml_processed": True},
    
    {"id": "M002", "title": "Mario Party Superstars", "category": "videojuegos", 
     "popularity": 0.75, "price": 59.99, "brand": "Nintendo",
     "features": ["mario", "party", "board-game", "multiplayer", "nintendo"],
     "embedding": [0.7, 0.3, 0.3, 0.7, 0.5, 0.3, 0.4, 0.5],
     "ml_processed": True},
    
    # Productos sin ML (para comparaci√≥n)
    {"id": "B001", "title": "PlayStation 5", "category": "consolas",
     "popularity": 0.85, "price": 499.99, "brand": "Sony",
     "features": ["console", "gaming", "ps5", "sony"],
     "ml_processed": False},
    
    {"id": "X001", "title": "Xbox Series X", "category": "consolas",
     "popularity": 0.8, "price": 499.99, "brand": "Microsoft",
     "features": ["console", "gaming", "xbox", "microsoft"],
     "ml_processed": False},
]

# Consultas de prueba con ground truth mejorado
TEST_QUERIES = [
    ("juegos de mario para nintendo", {"N002", "N004", "M001", "M002"}),
    ("nintendo switch juegos", {"N002", "N003", "N004", "N005", "M001", "M002"}),
    ("consola nintendo port√°til", {"N001"}),
    ("zelda para nintendo", {"N003"}),
    ("juegos de aventura para nintendo", {"N002", "N003"}),
    ("juegos multijugador para switch", {"N004", "M002"}),
    ("mario kart deluxe", {"N004"}),
    ("animal crossing new horizons", {"N005"}),
    ("consolas gaming", {"N001", "B001", "X001"}),  # Con productos sin ML
    ("videojuegos nintendo", {"N002", "N003", "N004", "N005", "M001", "M002"}),
]

# --- SIN√ìNIMOS Y COMPRENSI√ìN ML MEJORADA ---
SYNONYMS = {
    "mario": ["mario bros", "super mario", "mario bros."],
    "nintendo": ["switch", "nintendo switch", "ninten"],
    "juego": ["videojuego", "game", "video game"],
    "switch": ["nintendo switch", "consola h√≠brida", "switch nintendo"],
    "zelda": ["legend of zelda", "link", "zelda breath"],
    "aventura": ["adventure", "exploraci√≥n", "aventurero"],
    "multijugador": ["multiplayer", "co-op", "cooperative", "varios jugadores"],
    "consola": ["console", "system", "platform"],
    "gaming": ["videojuegos", "juegos", "gamer"],
}

# --- M√âTRICAS MEJORADAS CON AN√ÅLISIS ML ---
def precision_at_k(retrieved_lists: List[List[str]], ground_truth_sets: List[Set[str]], k: int = 5) -> float:
    """Precisi√≥n @k optimizada con an√°lisis ML."""
    precisions = []
    for retrieved, gt in zip(retrieved_lists, ground_truth_sets):
        if not gt:
            continue
        relevant = sum(1 for doc_id in retrieved[:k] if doc_id in gt)
        precisions.append(relevant / min(k, len(gt)))
    
    avg_precision = sum(precisions) / len(precisions) if precisions else 0.0
    
    # An√°lisis ML adicional
    if RL_MODELS_AVAILABLE:
        ml_products = sum(1 for r_list in retrieved_lists 
                         for doc_id in r_list[:k] 
                         if any(p.get('ml_processed', False) for p in TEST_PRODUCTS if p['id'] == doc_id))
        total_retrieved = sum(len(r[:k]) for r in retrieved_lists)
        if total_retrieved > 0:
            ml_ratio = ml_products / total_retrieved
            logger.debug(f"üìä ML Analysis: {ml_products}/{total_retrieved} productos con ML ({ml_ratio:.1%})")
    
    return avg_precision

def recall_at_k(retrieved_lists: List[List[str]], ground_truth_sets: List[Set[str]], k: int = 5) -> float:
    """Recall @k optimizado."""
    recalls = []
    for retrieved, gt in zip(retrieved_lists, ground_truth_sets):
        if not gt:
            continue
        relevant = sum(1 for doc_id in retrieved[:k] if doc_id in gt)
        recalls.append(relevant / len(gt))
    return sum(recalls) / len(recalls) if recalls else 0.0

def f1_score_at_k(retrieved_lists: List[List[str]], ground_truth_sets: List[Set[str]], k: int = 5) -> float:
    """F1 Score @k."""
    prec = precision_at_k(retrieved_lists, ground_truth_sets, k)
    rec = recall_at_k(retrieved_lists, ground_truth_sets, k)
    if prec + rec == 0:
        return 0.0
    return 2 * (prec * rec) / (prec + rec)

def hit_rate_at_k(retrieved_lists: List[List[str]], ground_truth_sets: List[Set[str]], k: int = 5) -> float:
    """Hit Rate @K."""
    hits = []
    for retrieved, gt in zip(retrieved_lists, ground_truth_sets):
        if not gt:
            continue
        hit = 1 if any(item in gt for item in retrieved[:k]) else 0
        hits.append(hit)
    return sum(hits) / len(hits) if hits else 0.0

def ndcg_at_k(retrieved_lists: List[List[str]], ground_truth_sets: List[Set[str]], k: int = 5) -> float:
    """NDCG @k optimizado."""
    ndcgs = []
    for retrieved, gt in zip(retrieved_lists, ground_truth_sets):
        if not gt:
            ndcgs.append(0.0)
            continue
        
        dcg = 0.0
        for i, doc_id in enumerate(retrieved[:k], 1):
            if doc_id in gt:
                dcg += 1.0 / math.log2(i + 1)
        
        ideal_ranking = list(gt)[:k]
        idcg = sum(1.0 / math.log2(i + 1) for i in range(1, min(len(ideal_ranking), k) + 1))
        
        ndcgs.append(dcg / idcg if idcg > 0 else 0.0)
    
    return sum(ndcgs) / len(ndcgs) if ndcgs else 0.0

def coverage(retrieved_lists: List[List[str]]) -> float:
    """Cobertura de productos recomendados con an√°lisis ML."""
    all_products = {p["id"] for p in TEST_PRODUCTS}
    recommended = set()
    for retrieved in retrieved_lists:
        recommended.update(retrieved[:5])
    
    coverage_score = len(recommended) / len(all_products) if all_products else 0.0
    
    # An√°lisis ML adicional
    ml_products = [p for p in TEST_PRODUCTS if p.get('ml_processed', False)]
    ml_recommended = [p_id for p_id in recommended 
                     if any(p['id'] == p_id and p.get('ml_processed', False) for p in TEST_PRODUCTS)]
    
    if ml_products:
        ml_coverage = len(ml_recommended) / len(ml_products)
        logger.debug(f"üìä ML Coverage: {len(ml_recommended)}/{len(ml_products)} productos ML ({ml_coverage:.1%})")
    
    return coverage_score

def diversity(retrieved_lists: List[List[str]]) -> float:
    """Diversidad de recomendaciones (1 - similitud promedio)."""
    if not retrieved_lists:
        return 0.0
    
    all_retrieved = []
    for r_list in retrieved_lists:
        all_retrieved.extend(r_list[:3])  # Usar top 3 para diversidad
    
    if len(all_retrieved) < 2:
        return 1.0
    
    # Calcular similitud basada en categor√≠as
    categories = []
    for p_id in all_retrieved:
        product = next((p for p in TEST_PRODUCTS if p['id'] == p_id), None)
        if product:
            categories.append(product.get('category', 'unknown'))
    
    # Calcular diversidad como 1 - (proporci√≥n de categor√≠as repetidas)
    if not categories:
        return 1.0
    
    unique_categories = set(categories)
    diversity_score = len(unique_categories) / len(categories)
    
    return diversity_score

def novelty(retrieved_lists: List[List[str]], popular_products: Set[str] = None) -> float:
    """Novelty - proporci√≥n de productos no populares recomendados."""
    if popular_products is None:
        # Definir populares como los top 3 por popularidad
        sorted_products = sorted(TEST_PRODUCTS, key=lambda x: x.get('popularity', 0), reverse=True)
        popular_products = {p['id'] for p in sorted_products[:3]}
    
    total_recommended = 0
    novel_recommended = 0
    
    for r_list in retrieved_lists:
        for p_id in r_list[:5]:
            total_recommended += 1
            if p_id not in popular_products:
                novel_recommended += 1
    
    return novel_recommended / total_recommended if total_recommended > 0 else 0.0

def personalization(retrieved_lists: List[List[str]]) -> float:
    """Personalizaci√≥n - similitud entre listas de recomendaciones."""
    if len(retrieved_lists) < 2:
        return 1.0
    
    similarities = []
    for i in range(len(retrieved_lists)):
        for j in range(i + 1, len(retrieved_lists)):
            set_i = set(retrieved_lists[i][:5])
            set_j = set(retrieved_lists[j][:5])
            
            if set_i and set_j:
                jaccard = len(set_i.intersection(set_j)) / len(set_i.union(set_j))
                similarities.append(jaccard)
    
    if not similarities:
        return 1.0
    
    avg_similarity = sum(similarities) / len(similarities)
    return 1.0 - avg_similarity  # M√°s personalizaci√≥n = menos similitud

# --- RETRIEVER INTELIGENTE CON ML ENTRENADO ---
class MLTrainedRetriever:
    """Retriever que usa modelos ML entrenados para mejor recuperaci√≥n."""
    
    def __init__(self, use_ml=True, use_rl_models=True):
        self.use_ml = use_ml
        self.use_rl_models = use_rl_models and RL_MODELS_AVAILABLE
        self.products = TEST_PRODUCTS
        self.query_cache = {}
        
        # Cargar embeddings precalculados
        self._load_embeddings()
        
        # Inicializar RL manager si est√° disponible
        if self.use_rl_models and rl_manager:
            self.rl_manager = rl_manager
            logger.info("‚úÖ Modelos RL entrenados cargados para scoring")
        else:
            self.rl_manager = None
        
        logger.info(f"ü§ñ MLTrainedRetriever inicializado - ML: {use_ml}, RL: {self.use_rl_models}")
    
    def _load_embeddings(self):
        """Carga embeddings de productos."""
        self.embeddings = {}
        for product in self.products:
            if 'embedding' in product:
                self.embeddings[product['id']] = np.array(product['embedding'], dtype=np.float32)
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """Calcula similitud coseno."""
        if vec1 is None or vec2 is None:
            return 0.0
        
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return np.dot(vec1, vec2) / (norm1 * norm2)
    
    def _get_query_embedding(self, query: str) -> np.ndarray:
        """Genera embedding para query (simulado para demo)."""
        # En producci√≥n, usar√≠a el mismo modelo que los productos
        query_words = query.lower().split()
        
        # Embedding simple basado en palabras clave
        embedding_dim = 8  # Mismo que productos de prueba
        embedding = np.zeros(embedding_dim, dtype=np.float32)
        
        keyword_weights = {
            'mario': [0.9, 0.1, 0.0, 0.8, 0.2, 0.0, 0.3, 0.6],
            'nintendo': [0.8, 0.0, 0.3, 0.9, 0.1, 0.2, 0.4, 0.5],
            'zelda': [0.7, 0.3, 0.4, 0.6, 0.8, 0.2, 0.1, 0.9],
            'switch': [0.6, 0.2, 0.5, 0.7, 0.0, 0.3, 0.6, 0.4],
            'juego': [0.5, 0.4, 0.6, 0.5, 0.3, 0.5, 0.2, 0.3],
            'consola': [0.4, 0.6, 0.7, 0.3, 0.2, 0.6, 0.1, 0.2],
        }
        
        for word in query_words:
            for keyword, weights in keyword_weights.items():
                if keyword in word or word in keyword:
                    embedding += np.array(weights)
        
        # Normalizar
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        return embedding
    
    def retrieve(self, query: str, top_k: int = 5) -> List[str]:
        """Recupera productos usando ML entrenado."""
        if self.use_ml and query in self.query_cache:
            return self.query_cache[query][:top_k]
        
        query_lower = query.lower()
        query_words = query_lower.split()
        scored_products = []
        
        # Obtener embedding de query si ML est√° habilitado
        query_embedding = None
        if self.use_ml:
            query_embedding = self._get_query_embedding(query)
        
        for product in self.products:
            score = 0.0
            title_lower = product["title"].lower()
            
            # 1. Puntuaci√≥n base (texto)
            for q_word in query_words:
                if q_word in title_lower:
                    score += 0.5
                if q_word in product.get("brand", "").lower():
                    score += 0.4
            
            # 2. Mejoras con ML embeddings
            if self.use_ml and query_embedding is not None:
                if product['id'] in self.embeddings:
                    product_embedding = self.embeddings[product['id']]
                    similarity = self._cosine_similarity(query_embedding, product_embedding)
                    score += similarity * 0.8  # Peso alto para embeddings
                
                # Comprensi√≥n de sin√≥nimos mejorada
                for q_word in query_words:
                    if q_word in SYNONYMS:
                        for synonym in SYNONYMS[q_word]:
                            if synonym in title_lower:
                                score += 0.4  # M√°s peso para sin√≥nimos
            
            # 3. Scoring RL si hay modelos entrenados
            if self.use_rl_models and self.rl_manager:
                rl_scores = self.rl_manager.apply_rl_scoring(query, [product['id']])
                if product['id'] in rl_scores:
                    score += rl_scores[product['id']] * 0.6  # Peso para RL
            
            # 4. Factores contextuales aprendidos
            if self.use_ml:
                # Detecci√≥n de intenci√≥n de compra
                if any(word in query_lower for word in ['comprar', 'comprar', 'precio', 'barato']):
                    if product.get('price', 0) < 100:
                        score += 0.3
                
                # Detecci√≥n de preferencias de plataforma
                if 'nintendo' in query_lower and product.get('brand') == 'Nintendo':
                    score += 0.4
                if 'playstation' in query_lower and product.get('brand') == 'Sony':
                    score += 0.4
                if 'xbox' in query_lower and product.get('brand') == 'Microsoft':
                    score += 0.4
            
            # 5. Popularidad y calidad
            score += product.get('popularity', 0) * 0.2
            
            # 6. Peque√±o ruido aleatorio (menor que sin ML)
            score += random.uniform(-0.02, 0.02) if self.use_ml else random.uniform(-0.05, 0.05)
            score = max(0.0, score)
            
            scored_products.append((score, product["id"]))
        
        # Ordenar y seleccionar
        scored_products.sort(key=lambda x: x[0], reverse=True)
        result = [pid for _, pid in scored_products[:top_k]]
        
        # Cache para ML
        if self.use_ml:
            self.query_cache[query_lower] = [pid for _, pid in scored_products]
        
        # Log detallado para debugging
        if logger.isEnabledFor(logging.DEBUG):
            top_scores = scored_products[:3]
            logger.debug(f"üîç Query: '{query}' - Top scores: {top_scores}")
        
        return result

# --- AGENTE RAG CON ML ENTRENADO ---
class MLTrainedRAGAgent:
    """Agente RAG que usa modelos ML entrenados."""
    
    def __init__(self, use_ml=True, use_rl_models=True):
        self.use_ml = use_ml
        self.use_rl_models = use_rl_models and RL_MODELS_AVAILABLE
        self.retriever = MLTrainedRetriever(use_ml=use_ml, use_rl_models=use_rl_models)
        
        # Cargar estad√≠sticas de entrenamiento
        self.training_stats = {}
        if self.use_rl_models and rl_manager:
            self.training_stats = rl_manager.get_training_stats()
            logger.info(f"üìä Estad√≠sticas de entrenamiento cargadas: {len(self.training_stats.get('model_types', {}))} tipos de modelos")
    
    def process_query(self, query: str) -> Tuple[str, List[str]]:
        """Procesa consulta con ML entrenado."""
        # Latencia m√°s realista con ML
        if self.use_ml:
            # ML tiene overhead computacional
            ml_delay = 0.02 if self.use_rl_models else 0.015
            time.sleep(ml_delay + random.uniform(0, 0.008))
        else:
            time.sleep(0.003 + random.uniform(0, 0.002))
        
        # Recuperar productos
        product_ids = self.retriever.retrieve(query, top_k=5)
        
        # Generar respuesta con informaci√≥n ML
        if self.use_ml:
            if self.use_rl_models:
                response = f"ü§ñ [ML+RL] Bas√°ndome en '{query}' y modelos entrenados, te recomiendo:"
            else:
                response = f"ü§ñ [ML] Bas√°ndome en '{query}' con embeddings ML, te recomiendo:"
            
            # A√±adir informaci√≥n de entrenamiento si est√° disponible
            if self.use_rl_models and self.training_stats:
                model_count = self.training_stats.get('total_models', 0)
                if model_count > 0:
                    response += f" (usando {model_count} modelos entrenados)"
        else:
            response = f"üîç Productos para: {query}"
        
        return response, product_ids

# --- EVALUACI√ìN MEJORADA CON AN√ÅLISIS ML ---
def evaluate_system(use_ml: bool = False, use_rl_models: bool = True) -> Dict[str, Any]:
    """Eval√∫a el sistema RAG con ML entrenado."""
    ml_text = "con ML" if use_ml else "sin ML"
    rl_text = "+RL" if (use_rl_models and use_ml) else ""
    
    logger.info(f"üìä Evaluando RAG {ml_text}{rl_text}...")
    
    agent = MLTrainedRAGAgent(use_ml=use_ml, use_rl_models=use_rl_models)
    queries = [q for q, _ in TEST_QUERIES]
    ground_truths = [gt for _, gt in TEST_QUERIES]
    
    start_time = time.time()
    all_retrieved = []
    
    for query in queries:
        _, product_ids = agent.process_query(query)
        all_retrieved.append(product_ids)
    
    elapsed_time = time.time() - start_time
    
    # M√©tricas base
    metrics = {
        "time_seconds": elapsed_time,
        "latency_ms": (elapsed_time / len(queries)) * 1000,
        "queries_count": len(queries),
        "precision@5": precision_at_k(all_retrieved, ground_truths, k=5),
        "recall@5": recall_at_k(all_retrieved, ground_truths, k=5),
        "f1@5": f1_score_at_k(all_retrieved, ground_truths, k=5),
        "hit_rate@5": hit_rate_at_k(all_retrieved, ground_truths, k=5),
        "ndcg@5": ndcg_at_k(all_retrieved, ground_truths, k=5),
        "coverage": coverage(all_retrieved),
        "diversity": diversity(all_retrieved),
        "novelty": novelty(all_retrieved),
        "personalization": personalization(all_retrieved),
        "config": {
            "ml_enabled": use_ml,
            "rl_models_enabled": use_rl_models and use_ml,
            "rl_models_available": RL_MODELS_AVAILABLE
        }
    }
    
    # An√°lisis ML adicional
    if use_ml:
        # Calcular proporci√≥n de productos ML recomendados
        ml_recommended = 0
        total_recommended = 0
        
        for retrieved in all_retrieved:
            for p_id in retrieved[:5]:
                total_recommended += 1
                product = next((p for p in TEST_PRODUCTS if p['id'] == p_id), None)
                if product and product.get('ml_processed', False):
                    ml_recommended += 1
        
        if total_recommended > 0:
            ml_ratio = ml_recommended / total_recommended
            metrics["ml_recommendation_ratio"] = ml_ratio
            
            # Calcular calidad de recomendaciones ML vs no-ML
            ml_precision = 0
            non_ml_precision = 0
            ml_count = 0
            non_ml_count = 0
            
            for retrieved, gt in zip(all_retrieved, ground_truths):
                for i, p_id in enumerate(retrieved[:5]):
                    product = next((p for p in TEST_PRODUCTS if p['id'] == p_id), None)
                    is_relevant = p_id in gt
                    
                    if product and product.get('ml_processed', False):
                        ml_precision += 1 if is_relevant else 0
                        ml_count += 1
                    else:
                        non_ml_precision += 1 if is_relevant else 0
                        non_ml_count += 1
            
            if ml_count > 0:
                metrics["ml_precision"] = ml_precision / ml_count
            if non_ml_count > 0:
                metrics["non_ml_precision"] = non_ml_precision / non_ml_count
    
    # A√±adir estad√≠sticas de modelos RL si est√°n disponibles
    if use_rl_models and RL_MODELS_AVAILABLE and rl_manager:
        rl_stats = rl_manager.get_training_stats()
        metrics["rl_training_stats"] = {
            "total_models": rl_stats.get('total_models', 0),
            "model_types": rl_stats.get('model_types', {}),
        }
    
    logger.info(f"‚úÖ {ml_text}{rl_text.upper()}: F1@5={metrics['f1@5']:.3f}, Prec@5={metrics['precision@5']:.3f}, "
               f"Latencia={metrics['latency_ms']:.1f}ms, ML Ratio={metrics.get('ml_recommendation_ratio', 0):.1%}")
    
    return metrics

# --- COMPARACI√ìN INTELIGENTE CON AN√ÅLISIS ML ---
def compare_results(results: Dict[str, Dict[str, Any]]) -> None:
    """Compara resultados con an√°lisis autom√°tico y ML."""
    print("\n" + "="*80)
    print("üéØ COMPARACI√ìN DETALLADA: RAG CON Y SIN ML (CON MODELOS ENTRENADOS)")
    print("="*80)
    
    # Tabla comparativa extendida
    headers = ["Sistema", "ML", "RL", "F1@5", "Prec@5", "Rec@5", "NDCG@5", "Cov", "Div", "Lat(ms)"]
    print(f"{headers[0]:<15} {headers[1]:<4} {headers[2]:<4} {headers[3]:<7} {headers[4]:<7} "
          f"{headers[5]:<7} {headers[6]:<7} {headers[7]:<5} {headers[8]:<5} {headers[9]:<8}")
    print("-"*80)
    
    for name, metrics in sorted(results.items()):
        ml_status = "‚úì" if metrics["config"]["ml_enabled"] else "‚úó"
        rl_status = "‚úì" if metrics["config"]["rl_models_enabled"] else "‚úó"
        
        print(f"{name:<15} {ml_status:<4} {rl_status:<4} {metrics['f1@5']:<7.3f} {metrics['precision@5']:<7.3f} "
              f"{metrics['recall@5']:<7.3f} {metrics['ndcg@5']:<7.3f} {metrics['coverage']:<5.2f} "
              f"{metrics['diversity']:<5.2f} {metrics['latency_ms']:<8.1f}")
    
    # An√°lisis de mejora
    if "rag_sin_ml" in results and "rag_con_ml" in results:
        without = results["rag_sin_ml"]
        with_ml = results["rag_con_ml"]
        
        print("\n" + "="*80)
        print("üìà AN√ÅLISIS DE MEJORAS CON ML Y MODELOS ENTRENADOS")
        print("="*80)
        
        improvements = []
        metrics_to_compare = [
            ("F1 Score @5", without["f1@5"], with_ml["f1@5"]),
            ("Precision @5", without["precision@5"], with_ml["precision@5"]),
            ("Recall @5", without["recall@5"], with_ml["recall@5"]),
            ("NDCG @5", without["ndcg@5"], with_ml["ndcg@5"]),
            ("Hit Rate @5", without["hit_rate@5"], with_ml["hit_rate@5"]),
            ("Coverage", without["coverage"], with_ml["coverage"]),
            ("Diversity", without["diversity"], with_ml["diversity"]),
        ]
        
        for metric_name, without_val, with_val in metrics_to_compare:
            if without_val > 0:
                improvement = ((with_val - without_val) / without_val * 100)
            else:
                improvement = 100.0 if with_val > 0 else 0.0
            
            improvements.append(improvement)
            
            # Selector de icono
            if improvement > 20:
                icon = "üöÄ"
            elif improvement > 10:
                icon = "üìà"
            elif improvement > 5:
                icon = "‚ÜóÔ∏è"
            elif improvement > 0:
                icon = "‚ÜóÔ∏è"
            elif improvement > -5:
                icon = "‚û°Ô∏è"
            elif improvement > -10:
                icon = "‚ÜòÔ∏è"
            else:
                icon = "‚ö†Ô∏è"
            
            print(f"{icon} {metric_name:<18}: {without_val:.3f} ‚Üí {with_val:.3f} ({improvement:+.1f}%)")
        
        # Latencia (trade-off)
        latency_diff = with_ml["latency_ms"] - without["latency_ms"]
        latency_icon = "‚ö†Ô∏è" if latency_diff > 20 else "‚è±Ô∏è" if latency_diff > 10 else "‚úÖ"
        print(f"{latency_icon} Latencia (trade-off): {without['latency_ms']:.1f}ms ‚Üí {with_ml['latency_ms']:.1f}ms "
              f"(+{latency_diff:.1f}ms)")
        
        # An√°lisis ML espec√≠fico
        if "ml_recommendation_ratio" in with_ml:
            ml_ratio = with_ml["ml_recommendation_ratio"]
            ml_icon = "ü§ñ" if ml_ratio > 0.7 else "üîç" if ml_ratio > 0.4 else "üìä"
            print(f"{ml_icon} Ratio ML recomendado: {ml_ratio:.1%}")
            
            if "ml_precision" in with_ml and "non_ml_precision" in with_ml:
                ml_precision = with_ml["ml_precision"]
                non_ml_precision = with_ml["non_ml_precision"]
                
                if ml_precision > non_ml_precision:
                    print(f"‚úÖ ML m√°s preciso: {ml_precision:.3f} vs {non_ml_precision:.3f} "
                          f"(+{(ml_precision-non_ml_precision)*100:.1f}%)")
                else:
                    print(f"‚ö†Ô∏è  ML menos preciso: {ml_precision:.3f} vs {non_ml_precision:.3f}")
        
        # An√°lisis RL si est√° disponible
        if with_ml["config"]["rl_models_enabled"] and "rl_training_stats" in with_ml:
            rl_stats = with_ml["rl_training_stats"]
            print(f"\nüß† MODELOS RL ENTRENADOS:")
            print(f"   ‚Ä¢ Total modelos: {rl_stats.get('total_models', 0)}")
            for model_type, count in rl_stats.get('model_types', {}).items():
                print(f"   ‚Ä¢ {model_type}: {count} modelos")
        
        # Recomendaci√≥n inteligente
        avg_improvement = sum(improvements) / len(improvements)
        
        print("\n" + "-"*80)
        if avg_improvement > 15 and latency_diff < 15:
            if with_ml["config"]["rl_models_enabled"]:
                print("üöÄ RECOMENDACI√ìN: IMPLEMENTAR ML CON MODELOS RL")
                print("   ‚Ä¢ Mejora significativa en calidad")
                print("   ‚Ä¢ Overhead de latencia aceptable")
                print("   ‚Ä¢ Beneficio neto: MUY ALTO (usando modelos entrenados)")
            else:
                print("üéØ RECOMENDACI√ìN: IMPLEMENTAR ML")
                print("   ‚Ä¢ Mejora significativa en calidad")
                print("   ‚Ä¢ Overhead de latencia aceptable")
                print("   ‚Ä¢ Beneficio neto: ALTO")
        elif avg_improvement > 8 and latency_diff < 25:
            print("‚úÖ RECOMENDACI√ìN: CONSIDERAR ML")
            print("   ‚Ä¢ Mejora moderada en calidad")
            print("   ‚Ä¢ Trade-off razonable")
            print("   ‚Ä¢ Beneficio neto: MODERADO")
        else:
            print("‚ö° RECOMENDACI√ìN: MANTENER SIN ML")
            print("   ‚Ä¢ Mejora limitada o overhead alto")
            print("   ‚Ä¢ Beneficio neto: BAJO")
        
        print(f"üìä Mejora promedio: {avg_improvement:+.1f}%")
        
        # An√°lisis adicional para RL
        if with_ml["config"]["rl_models_enabled"] and RL_MODELS_AVAILABLE:
            print(f"\nüîç AN√ÅLISIS RL ENTRENADO:")
            print(f"   ‚Ä¢ Modelos disponibles: {len(rl_manager.models)}")
            print(f"   ‚Ä¢ √öltimo entrenamiento: {rl_manager.config.get('last_training', 'desconocido') if hasattr(rl_manager, 'config') else 'desconocido'}")
            print(f"   ‚Ä¢ Datos de entrenamiento: {rl_manager.config.get('training_samples', 0) if hasattr(rl_manager, 'config') else 0} muestras")

# --- MAIN MEJORADO ---
def main():
    """Funci√≥n principal con an√°lisis ML entrenado."""
    parser = argparse.ArgumentParser(
        description="Evaluador con ML entrenado para comparar RAG con y sin ML",
        epilog="""
Ejemplos:
  python deepeval_ml_trained.py                  # Comparaci√≥n completa
  python deepeval_ml_trained.py --ml-only        # Solo RAG con ML entrenado
  python deepeval_ml_trained.py --no-ml          # Solo RAG sin ML
  python deepeval_ml_trained.py --no-rl          # ML sin modelos RL
  python deepeval_ml_trained.py --show-rl-stats  # Mostrar estad√≠sticas RL
        """
    )
    
    parser.add_argument("--ml-only", action="store_true", help="Evaluar solo RAG con ML")
    parser.add_argument("--no-ml", action="store_true", help="Evaluar solo RAG sin ML")
    parser.add_argument("--no-rl", action="store_true", help="Usar ML pero sin modelos RL")
    parser.add_argument("--show-rl-stats", action="store_true", help="Mostrar estad√≠sticas de modelos RL")
    parser.add_argument("--output", type=str, default="evaluation_results_ml_trained.json", help="Archivo de salida")
    parser.add_argument("--seed", type=int, default=42, help="Semilla para reproducibilidad")
    parser.add_argument("-v", "--verbose", action="store_true", help="Mostrar logs detallados")
    parser.add_argument("--debug", action="store_true", help="Modo debug (m√°s detalles)")
    
    args = parser.parse_args()
    
    # Configurar semilla
    random.seed(args.seed)
    np.random.seed(args.seed)
    
    # Configurar logging
    if args.verbose or args.debug:
        logger.setLevel(logging.DEBUG)
    
    # Mostrar estad√≠sticas RL si se solicita
    if args.show_rl_stats and rl_manager:
        print("\n" + "="*60)
        print("üß† ESTAD√çSTICAS DE MODELOS RL ENTRENADOS")
        print("="*60)
        
        stats = rl_manager.get_training_stats()
        print(f"üìä Total modelos: {stats.get('total_models', 0)}")
        
        if stats.get('model_types'):
            print("\nüì¶ Tipos de modelos:")
            for model_type, count in stats['model_types'].items():
                print(f"   ‚Ä¢ {model_type}: {count}")
        
        if stats.get('training_info'):
            print("\nüéØ Informaci√≥n de entrenamiento:")
            for model_name, info in stats['training_info'].items():
                print(f"   ‚Ä¢ {model_name}:")
                for key, value in info.items():
                    if isinstance(value, (int, float)):
                        print(f"     - {key}: {value}")
        
        if stats.get('global_config'):
            print("\n‚öôÔ∏è Configuraci√≥n global:")
            for key, value in stats['global_config'].items():
                print(f"   ‚Ä¢ {key}: {value}")
        
        print()
        
        if args.ml_only or args.no_ml:
            # Si solo se pidieron stats, salir
            return
    
    logger.info("üöÄ INICIANDO EVALUACI√ìN CON ML ENTRENADO")
    logger.info(f"üìã {len(TEST_PRODUCTS)} productos ({sum(1 for p in TEST_PRODUCTS if p.get('ml_processed', False))} con ML)")
    logger.info(f"üîç {len(TEST_QUERIES)} consultas de prueba")
    logger.info(f"ü§ñ Modelos RL disponibles: {RL_MODELS_AVAILABLE}")
    
    if RL_MODELS_AVAILABLE and rl_manager:
        stats = rl_manager.get_training_stats()
        logger.info(f"üìä Modelos RL cargados: {stats.get('total_models', 0)}")
    
    # Ejecutar evaluaciones
    results = {}
    
    if args.ml_only:
        logger.info("üî¨ Evaluando solo RAG CON ML entrenado...")
        use_rl = not args.no_rl
        results["rag_con_ml"] = evaluate_system(use_ml=True, use_rl_models=use_rl)
    elif args.no_ml:
        logger.info("üî¨ Evaluando solo RAG SIN ML...")
        results["rag_sin_ml"] = evaluate_system(use_ml=False)
    else:
        logger.info("1Ô∏è‚É£ Evaluando RAG SIN ML...")
        results["rag_sin_ml"] = evaluate_system(use_ml=False)
        logger.info("2Ô∏è‚É£ Evaluando RAG CON ML...")
        results["rag_con_ml"] = evaluate_system(use_ml=True, use_rl_models=not args.no_rl)
    
    # Mostrar comparaci√≥n si hay ambos
    if len(results) > 1:
        compare_results(results)
    
    # Guardar resultados
    output_data = {
        "timestamp": time.time(),
        "test_queries": len(TEST_QUERIES),
        "test_products": len(TEST_PRODUCTS),
        "test_products_with_ml": sum(1 for p in TEST_PRODUCTS if p.get('ml_processed', False)),
        "rl_models_available": RL_MODELS_AVAILABLE,
        "rl_models_count": len(rl_manager.models) if rl_manager else 0,
        "evaluation_seed": args.seed,
        "results": results
    }
    
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    logger.info(f"üíæ Resultados guardados en: {args.output}")
    
    # Resumen final
    print("\n" + "="*70)
    print("üéØ RESUMEN EJECUTIVO - ML ENTRENADO")
    print("="*70)
    
    for name, metrics in results.items():
        ml_status = "CON ML" if metrics["config"]["ml_enabled"] else "SIN ML"
        rl_status = "+RL" if metrics["config"]["rl_models_enabled"] else ""
        
        print(f"\nüìä SISTEMA: {name.upper()} ({ml_status}{rl_status})")
        print(f"   ‚Ä¢ F1 Score: {metrics['f1@5']:.3f}")
        print(f"   ‚Ä¢ Precisi√≥n: {metrics['precision@5']:.3f}")
        print(f"   ‚Ä¢ Recall: {metrics['recall@5']:.3f}")
        print(f"   ‚Ä¢ NDCG: {metrics['ndcg@5']:.3f}")
        print(f"   ‚Ä¢ Latencia: {metrics['latency_ms']:.1f}ms")
        
        if "ml_recommendation_ratio" in metrics:
            print(f"   ‚Ä¢ Ratio ML: {metrics['ml_recommendation_ratio']:.1%}")
        
        if "diversity" in metrics:
            print(f"   ‚Ä¢ Diversidad: {metrics['diversity']:.2f}")
    
    print(f"\nüìÑ Resultados completos: {args.output}")
    print("‚úÖ Evaluaci√≥n completada")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Evaluaci√≥n interrumpida")
        sys.exit(0)
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)