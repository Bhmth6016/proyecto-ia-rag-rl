# src/core/init.py
import hashlib
from pathlib import Path
from typing import List, Optional, Dict, Any
from src.core.data.loader import DataLoader
from src.core.data.product import Product
from src.core.rag.basic.retriever import Retriever
from src.core.config import settings
import logging
import google.generativeai as genai
logger = logging.getLogger(__name__)
from dotenv import load_dotenv

load_dotenv()

class SystemInitializer:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if not self._initialized:
            self._products = None
            self._retriever = None
            self._initialized = True
            self.llm_model = genai.GenerativeModel("gemini-1.5-flash")
            
            # ðŸ”¥ NUEVO: ConfiguraciÃ³n ML
            self.ml_enabled = getattr(settings, "ML_ENABLED", False)
            self.ml_features = getattr(settings, "ML_FEATURES", ["category", "entities"])
            self.ml_models = {}
            
            # ðŸ”¥ NUEVO: ConfiguraciÃ³n especÃ­fica para CollaborativeFilter
            self.collaborative_ml_config = {
                'use_ml_features': getattr(settings, "COLLABORATIVE_ML_ENABLED", True),
                'ml_weight': getattr(settings, "ML_WEIGHT", 0.3),
                'min_similar_users': getattr(settings, "MIN_SIMILAR_USERS", 3),
                'ml_embedding_dim': getattr(settings, "ML_EMBEDDING_DIM", 768)
            }
            
            # ðŸ”¥ NUEVO: ConfiguraciÃ³n para embeddings
            self.embedding_config = {
                'use_sentence_transformers': getattr(settings, "USE_SENTENCE_TRANSFORMERS", True),
                'embedding_model': getattr(settings, "EMBEDDING_MODEL_ML", "all-MiniLM-L6-v2"),
                'cache_embeddings': getattr(settings, "CACHE_EMBEDDINGS", True)
            }
            
            # ðŸ”¥ NUEVO: Inicializar modelos ML si estÃ¡n habilitados
            if self.ml_enabled:
                self._initialize_ml_components()
            
            logger.info(f"âœ… SystemInitializer creado - ML Enabled: {self.ml_enabled}")
            logger.info(f"ðŸ”§ ML Features: {self.ml_features}")
            logger.info(f"ðŸ¤ Collaborative ML Config: {self.collaborative_ml_config}")

    def _initialize_ml_components(self) -> None:
        """Inicializa componentes ML si estÃ¡n habilitados en configuraciÃ³n"""
        try:
            logger.info("ðŸš€ Inicializando componentes ML...")
            
            # ðŸ”¥ NUEVO: Inicializar embeddings para ML si es necesario
            if self.embedding_config['use_sentence_transformers']:
                self._initialize_sentence_transformer()
            
            # ðŸ”¥ NUEVO: Verificar si hay modelos ML pre-entrenados para cargar
            self._load_pretrained_models()
            
            # ðŸ”¥ NUEVO: Inicializar cachÃ© de embeddings
            if self.embedding_config['cache_embeddings']:
                self._initialize_embedding_cache()
                
            logger.info("âœ… Componentes ML inicializados correctamente")
            
        except Exception as e:
            logger.error(f"âŒ Error inicializando componentes ML: {e}")
            # Desactivar ML si hay error en inicializaciÃ³n
            self.ml_enabled = False

    def _initialize_sentence_transformer(self) -> None:
        """Inicializa modelo de Sentence Transformers para embeddings"""
        try:
            # ImportaciÃ³n condicional para evitar dependencias innecesarias
            from sentence_transformers import SentenceTransformer
            
            logger.info(f"ðŸ”§ Cargando Sentence Transformer: {self.embedding_config['embedding_model']}")
            self.ml_models['sentence_transformer'] = SentenceTransformer(
                self.embedding_config['embedding_model']
            )
            logger.info(f"âœ… Sentence Transformer cargado: {self.embedding_config['embedding_model']}")
            
        except ImportError:
            logger.warning("âš ï¸  Sentence Transformers no estÃ¡ instalado. Usando embeddings bÃ¡sicos.")
            self.embedding_config['use_sentence_transformers'] = False
        except Exception as e:
            logger.error(f"âŒ Error cargando Sentence Transformer: {e}")
            self.embedding_config['use_sentence_transformers'] = False

    def _load_pretrained_models(self) -> None:
        """Carga modelos ML pre-entrenados si existen"""
        models_path = Path(settings.MODELS_DIR) if hasattr(settings, 'MODELS_DIR') else Path("models")
        
        if models_path.exists():
            logger.info(f"ðŸ” Buscando modelos pre-entrenados en: {models_path}")
            
            # Lista de modelos a buscar
            model_files = {
                'category_classifier': models_path / "category_classifier.pkl",
                'sentiment_analyzer': models_path / "sentiment_analyzer.pkl",
                'similarity_model': models_path / "similarity_model.pkl"
            }
            
            for model_name, model_path in model_files.items():
                if model_path.exists():
                    try:
                        import pickle
                        with open(model_path, 'rb') as f:
                            self.ml_models[model_name] = pickle.load(f)
                        logger.info(f"âœ… Modelo {model_name} cargado desde {model_path}")
                    except Exception as e:
                        logger.error(f"âŒ Error cargando modelo {model_name}: {e}")

    def _initialize_embedding_cache(self) -> None:
        """Inicializa cachÃ© de embeddings"""
        try:
            import hashlib
            import json
            from pathlib import Path
            
            cache_dir = Path(getattr(settings, "EMBEDDING_CACHE_DIR", "data/cache/embeddings"))
            cache_dir.mkdir(parents=True, exist_ok=True)
            
            self.embedding_cache_path = cache_dir / "embeddings_cache.json"
            
            if self.embedding_cache_path.exists():
                with open(self.embedding_cache_path, 'r', encoding='utf-8') as f:
                    self.embedding_cache = json.load(f)
                logger.info(f"ðŸ“ CachÃ© de embeddings cargada: {len(self.embedding_cache)} entradas")
            else:
                self.embedding_cache = {}
                logger.info("ðŸ“ CachÃ© de embeddings inicializada vacÃ­a")
                
        except Exception as e:
            logger.error(f"âŒ Error inicializando cachÃ© de embeddings: {e}")
            self.embedding_cache = {}

    def get_ml_embedding(self, text: str) -> Optional[List[float]]:
        """Obtiene embedding para texto usando modelo ML configurado"""
        if not self.ml_enabled or not self.embedding_config['use_sentence_transformers']:
            return None
            
        try:
            # ðŸ”¥ NUEVO: Verificar cachÃ© primero
            cache_key = hashlib.md5(text.encode('utf-8')).hexdigest()
            
            if self.embedding_config['cache_embeddings'] and cache_key in self.embedding_cache:
                return self.embedding_cache[cache_key]
            
            # ðŸ”¥ NUEVO: Calcular embedding si no estÃ¡ en cachÃ©
            if 'sentence_transformer' in self.ml_models:
                embedding = self.ml_models['sentence_transformer'].encode(text).tolist()
                
                # ðŸ”¥ NUEVO: Guardar en cachÃ©
                if self.embedding_config['cache_embeddings']:
                    self.embedding_cache[cache_key] = embedding
                    self._save_embedding_cache()
                
                return embedding
                
        except Exception as e:
            logger.error(f"âŒ Error obteniendo embedding ML: {e}")
            
        return None

    def _save_embedding_cache(self) -> None:
        """Guarda la cachÃ© de embeddings en disco"""
        try:
            import json
            
            with open(self.embedding_cache_path, 'w', encoding='utf-8') as f:
                json.dump(self.embedding_cache, f)
                
        except Exception as e:
            logger.error(f"âŒ Error guardando cachÃ© de embeddings: {e}")

    @property
    def products(self) -> List[Product]:
        if self._products is None:
            self._load_products()
        return self._products

    @property
    def retriever(self) -> Retriever:
        if self._retriever is None:
            self._initialize_retriever()
        return self._retriever

    @property
    def loader(self) -> DataLoader:
        if not hasattr(self, '_loader'):
            self._loader = DataLoader(
                raw_dir=settings.RAW_DIR,
                processed_dir=settings.PROC_DIR
            )
        return self._loader

    def _load_products(self) -> None:
        """Load products with caching."""
        loader = DataLoader(
            raw_dir=settings.RAW_DIR,
            processed_dir=settings.PROC_DIR,
            cache_enabled=settings.CACHE_ENABLED
        )
        self._products = loader.load_data()

    def _initialize_retriever(self) -> None:
        """Initialize retriever and build index if needed."""
        logger.info(f"Initializing retriever at {settings.VECTOR_INDEX_PATH}")
        
        # Asegura que el directorio existe
        index_path = Path(settings.VECTOR_INDEX_PATH)
        index_path.parent.mkdir(parents=True, exist_ok=True)
        
        self._retriever = Retriever(
            index_path=settings.VECTOR_INDEX_PATH,
            embedding_model=settings.EMBEDDING_MODEL,
            device=settings.DEVICE
        )
        
        # Verifica si el Ã­ndice existe
        if not self._retriever.index_exists():
            logger.info("Index not found, building...")
            if not hasattr(self, '_products') or not self._products:
                self._load_products()
            self._retriever.build_index(self._products)
    
    # ðŸ”¥ NUEVO: MÃ©todos para obtener configuraciÃ³n ML
    def get_ml_config(self) -> Dict[str, Any]:
        """Retorna la configuraciÃ³n ML completa"""
        return {
            'ml_enabled': self.ml_enabled,
            'ml_features': self.ml_features,
            'collaborative_ml_config': self.collaborative_ml_config,
            'embedding_config': self.embedding_config
        }
    
    def is_ml_feature_enabled(self, feature: str) -> bool:
        """Verifica si una feature ML especÃ­fica estÃ¡ habilitada"""
        return feature in self.ml_features
    
    def update_ml_config(self, config_updates: Dict[str, Any]) -> None:
        """Actualiza configuraciÃ³n ML dinÃ¡micamente"""
        try:
            if 'ml_enabled' in config_updates:
                self.ml_enabled = config_updates['ml_enabled']
                
            if 'ml_features' in config_updates:
                self.ml_features = config_updates['ml_features']
                
            if 'collaborative_ml_config' in config_updates:
                self.collaborative_ml_config.update(config_updates['collaborative_ml_config'])
                
            if 'embedding_config' in config_updates:
                self.embedding_config.update(config_updates['embedding_config'])
                
            logger.info(f"ðŸ”§ ConfiguraciÃ³n ML actualizada: {self.get_ml_config()}")
            
        except Exception as e:
            logger.error(f"âŒ Error actualizando configuraciÃ³n ML: {e}")


def get_system() -> SystemInitializer:
    """Global access point for initialized system."""
    return SystemInitializer()