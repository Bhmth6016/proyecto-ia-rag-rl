# src/core/rag/advanced/WorkingRAGAgent.py
"""
WorkingRAGAgent - Agente RAG avanzado con configuraci√≥n ML centralizada.
Usa ProductReference y settings como √∫nica fuente de verdad.
"""

import logging
from typing import List, Optional, Dict, Any, Tuple, Callable, Union, Sequence
from dataclasses import dataclass, field
from enum import Enum
import time
import torch
from pathlib import Path
import numpy as np
import random
from collections import defaultdict
# Importar configuraci√≥n centralizada
from src.core.config import settings, get_settings
from src.core.data.product import Product
from src.core.data.product_reference import ProductReference, create_ml_enhanced_reference

logger = logging.getLogger(__name__)


class RAGMode(Enum):
    """Modos de operaci√≥n del RAG."""
    BASIC = "basic"
    HYBRID = "hybrid"
    ML_ENHANCED = "ml_enhanced"
    LLM_ENHANCED = "llm_enhanced"


@dataclass
class RAGConfig:
    """Configuraci√≥n del agente RAG."""
    # Modo de operaci√≥n
    mode: RAGMode = RAGMode.HYBRID
    
    # Configuraci√≥n de recuperaci√≥n
    enable_reranking: bool = True
    max_retrieved: int = 15
    max_final: int = 5
    
    # Configuraci√≥n ML (se hereda de settings)
    ml_enabled: bool = field(default_factory=lambda: settings.ML_ENABLED)
    ml_features: List[str] = field(default_factory=lambda: list(settings.ML_FEATURES))
    use_ml_embeddings: bool = field(default_factory=lambda: settings.ML_ENABLED and 'embedding' in settings.ML_FEATURES)
    ml_embedding_weight: float = field(default_factory=lambda: settings.ML_WEIGHT)
    
    # Configuraci√≥n LLM
    local_llm_enabled: bool = field(default_factory=lambda: settings.LOCAL_LLM_ENABLED)
    local_llm_model: str = field(default_factory=lambda: settings.LOCAL_LLM_MODEL)
    use_llm_for_reranking: bool = False
    
    # Configuraci√≥n de dominio
    domain: str = "general"
    use_advanced_features: bool = True
    
    # Ponderaciones para scoring h√≠brido
    semantic_weight: float = 0.6
    popularity_weight: float = 0.2
    diversity_weight: float = 0.1
    freshness_weight: float = 0.1



class SimpleRLBandit:
    """
    Implementaci√≥n m√≠nima de RL (Multi-Armed Bandit) para el Paper.
    - Estado: (Impl√≠cito) Query actual
    - Acci√≥n: Reordenar/Seleccionar producto
    - Recompensa: Feedback de usuario (+1/-1)
    - Pol√≠tica: Epsilon-Greedy
    """
    def __init__(self, epsilon=0.2, alpha=0.1):
        self.q_values = defaultdict(float) # Tabla Q(s,a) simplificada a Q(product_id)
        self.epsilon = epsilon  # Factor de exploraci√≥n
        self.alpha = alpha      # Tasa de aprendizaje
        self.counts = defaultdict(int) # Para estad√≠sticas

    def rank(self, products: List[Any]) -> List[Any]:
        """Aplica pol√≠tica Epsilon-Greedy para reordenar."""
        if not products:
            return []
            
        # Exploraci√≥n: Aleatorizar orden (descubrir nuevos productos buenos)
        if random.random() < self.epsilon:
            shuffled = products.copy()
            random.shuffle(shuffled)
            return shuffled
            
        # Explotaci√≥n: Ordenar por valor Q aprendido (usar lo que sabemos que gusta)
        # Se combina con el score original para mantener relevancia sem√°ntica
        return sorted(
            products, 
            key=lambda p: self.get_q_value(p) + (getattr(p, 'score', 0) * 0.5), 
            reverse=True
        )

    def get_q_value(self, product) -> float:
        """Obtiene valor Q actual para un producto."""
        pid = getattr(product, 'id', str(product))
        return self.q_values[pid]

    def update(self, product_id: str, reward: float):
        """Actualizaci√≥n Q-Learning: Q(a) = Q(a) + alpha * (r - Q(a))"""
        old_val = self.q_values[product_id]
        # Ecuaci√≥n fundamental de RL (Temporal Difference)
        new_val = old_val + self.alpha * (reward - old_val)
        self.q_values[product_id] = new_val
        self.counts[product_id] += 1


class WorkingAdvancedRAGAgent:
    """
    Agente RAG avanzado que usa configuraci√≥n ML centralizada
    y ProductReference para manejo consistente.
    """
    
    def __init__(self, config: Optional[RAGConfig] = None):
        # üî• Usar configuraci√≥n centralizada
        self.settings = get_settings()
        
        # Configuraci√≥n del agente
        self.config = config or RAGConfig()

        # üî• NUEVO: Inicializar motor RL Simple
        self.rl_bandit = SimpleRLBandit(epsilon=0.2, alpha=0.1)
        
        # Componentes del sistema (lazy loaded)
        self._retriever = None
        self._llm_client = None
        self._embedding_model = None
        
        # Cache para embeddings de queries
        self._query_cache: Dict[str, List[float]] = {}
        
        # Inicializar logger
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # üî• NUEVO: Pipeline RLHF
        self.rlhf_pipeline = None
        self.rlhf_model = None
        self._init_rlhf()
        
        # üî• NUEVO: Inicializar Collaborative Filter
        self._collaborative_filter = None
        self._init_collaborative_filter()
        
        self.logger.info(f"üöÄ WorkingAdvancedRAGAgent inicializado")
        self.logger.info(f"   ‚Ä¢ Modo: {self.config.mode.value}")
        self.logger.info(f"   ‚Ä¢ ML: {'‚úÖ' if self.config.ml_enabled else '‚ùå'}")
        self.logger.info(f"   ‚Ä¢ LLM Local: {'‚úÖ' if self.config.local_llm_enabled else '‚ùå'}")
        self.logger.info(f"   ‚Ä¢ RLHF: {'‚úÖ' if self.rlhf_pipeline else '‚ùå'}")
        self.logger.info(f"   ‚Ä¢ Collaborative Filter: {'‚úÖ' if self._collaborative_filter else '‚ùå'}")
        self.logger.info(f"üß† RL Bandit Simple: ‚úÖ ACTIVO")

    def _init_rlhf(self):
        """Inicializar componente RLHF si est√° habilitado"""
        try:
            from src.core.rag.advanced.train_pipeline import RLHFTrainingPipeline
            self.rlhf_pipeline = RLHFTrainingPipeline()
            
            # Intentar cargar modelo existente
            if (Path("data/models/rlhf_model") / "pytorch_model.bin").exists():
                self.rlhf_model = self.rlhf_pipeline.load_model()
                logger.info("üß† RLHF integrado (modelo cargado)")
            else:
                logger.info("üß† RLHF integrado (sin modelo entrenado)")
                
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è RLHF no disponible: {e}")
            self.rlhf_pipeline = None
    
    def _init_collaborative_filter(self):
        """Inicializar Collaborative Filter si est√° habilitado"""
        try:
            from src.core.rag.advanced.collaborative_filter import CollaborativeFilter
            from src.core.data.user_manager import UserManager
            from src.core.data.product_service import ProductService
            
            # Obtener gestor de usuarios
            user_manager = UserManager()
            
            # Usar ProductService real
            product_service = ProductService()
            
            # Crear filtro colaborativo con servicio real
            self._collaborative_filter = CollaborativeFilter(
                user_manager=user_manager,
                product_service=product_service,
                use_ml_features=self.config.ml_enabled
            )
            
            logger.info("ü§ù Collaborative Filter integrado (con ProductService)")
            
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è Collaborative Filter no disponible: {e}")
            self._init_simple_collaborative_filter()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error inicializando Collaborative Filter: {e}")
    
    def _init_simple_collaborative_filter(self):
        """Inicializar filtro colaborativo simple como fallback"""
        try:
            from src.core.rag.advanced.collaborative_filter import CollaborativeFilter
            from src.core.data.user_manager import UserManager
            
            user_manager = UserManager()
            
            # Crear filtro sin product_service
            self._collaborative_filter = CollaborativeFilter(
                user_manager=user_manager,
                product_service=None,
                use_ml_features=self.config.ml_enabled
            )
            
            logger.info("ü§ù Collaborative Filter simple inicializado (fallback)")
        except Exception as e:
            logger.error(f"‚ùå Error inicializando filtro simple: {e}")
            self._collaborative_filter = None
    
    # --------------------------------------------------
    # Propiedades lazy
    # --------------------------------------------------
    
    @property
    def retriever(self):
        """Retriever vectorial (lazy loading)."""
        if self._retriever is None:
            try:
                from src.core.rag.basic.retriever import Retriever
                self._retriever = Retriever(
                    index_path=settings.VECTOR_INDEX_PATH,
                    embedding_model=settings.EMBEDDING_MODEL,
                    device=settings.DEVICE
                )
                self.logger.info(f"‚úÖ Retriever inicializado: {settings.EMBEDDING_MODEL}")
            except ImportError as e:
                self.logger.error(f"‚ùå No se pudo cargar Retriever: {e}")
                raise
        return self._retriever
    
    @property
    def llm_client(self):
        """Cliente LLM local (lazy loading)."""
        if self._llm_client is None and self.config.local_llm_enabled:
            try:
                from src.core.llm.local_llm import LocalLLMClient
                self._llm_client = LocalLLMClient(
                    model=settings.LOCAL_LLM_MODEL,
                    endpoint=settings.LOCAL_LLM_ENDPOINT,
                    temperature=settings.LOCAL_LLM_TEMPERATURE,
                    timeout=settings.LOCAL_LLM_TIMEOUT
                )
                self.logger.info(f"‚úÖ LLM Client inicializado: {settings.LOCAL_LLM_MODEL}")
            except ImportError as e:
                self.logger.warning(f"‚ö†Ô∏è No se pudo cargar LocalLLMClient: {e}")
            except Exception as e:
                self.logger.error(f"‚ùå Error inicializando LLM: {e}")
        return self._llm_client
    
    @property
    def embedding_model(self):
        """Modelo de embeddings (lazy loading)."""
        if self._embedding_model is None and self.config.use_ml_embeddings:
            try:
                from sentence_transformers import SentenceTransformer
                self._embedding_model = SentenceTransformer(settings.ML_EMBEDDING_MODEL)
                self.logger.info(f"‚úÖ Embedding Model cargado: {settings.ML_EMBEDDING_MODEL}")
            except ImportError as e:
                self.logger.warning(f"‚ö†Ô∏è SentenceTransformer no disponible: {e}")
            except Exception as e:
                self.logger.error(f"‚ùå Error cargando embedding model: {e}")
        return self._embedding_model
    
    # --------------------------------------------------
    # M√©todos principales
    # --------------------------------------------------

    def process_query(self, query: str, user_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Procesa una consulta completa usando RAG avanzado.
        """
        start_time = time.time()
        
        try:
            self.logger.info(f"üîç Procesando consulta: '{query[:50]}...'")
            
            # 1. B√∫squeda sem√°ntica inicial
            initial_results = self._semantic_search(query)

            # üîç Opcional: imprimir resultados encontrados como solicitaban
            self.logger.debug(f"Encontrados {len(initial_results)} resultados iniciales")
            for i, ref in enumerate(initial_results[:3]):
                self.logger.debug(f"{i+1}. {ref.title[:50]}... (score: {ref.score})")

            # 2. Enrich con ML si est√° habilitado
            ml_enhanced_results = (
                self._enhance_with_ml(initial_results, query)
                if self.config.ml_enabled else initial_results
            )

            # 3. Re-ranking final
            final_results = (
                self._rerank_results(ml_enhanced_results, query, user_id)
                if self.config.enable_reranking else
                ml_enhanced_results[:self.config.max_final]
            )

            # 4. Generaci√≥n de respuesta con LLM
            answer = self._generate_answer(query, final_results)

            # 5. M√©tricas
            processing_time = time.time() - start_time

            response = {
                "query": query,
                "answer": answer,
                "products": final_results,
                "stats": {
                    "processing_time": round(processing_time, 2),
                    "initial_results": len(initial_results),
                    "final_results": len(final_results),
                    "ml_enhanced": self.config.ml_enabled,
                    "reranking_enabled": self.config.enable_reranking,
                }
            }

            self.logger.info(f"‚úÖ Consulta procesada en {processing_time:.2f}s")
            return response

        except Exception as e:
            import traceback
            self.logger.error(f"‚ùå Error procesando consulta: {e}")
            self.logger.error(traceback.format_exc())

            return {
                "query": query,
                "answer": "Lo siento, hubo un error procesando tu consulta.",
                "products": [],
                "error": str(e)
            }
    
    def _semantic_search(self, query: str) -> List[ProductReference]:
        """B√∫squeda sem√°ntica usando embeddings."""
        try:
            # üî• SIMPLIFICADO: Usar search() del retriever
            raw_results = self.retriever.search(
                query=query,
                k=self.config.max_retrieved
            )
            
            product_references = []
            for product in raw_results:
                try:
                    # üî• Validar que el producto sea v√°lido
                    if not product or not hasattr(product, 'title'):
                        continue
                    
                    # üî• Asegurar que el t√≠tulo no sea None
                    if not product.title:
                        continue
                    
                    # Calcular score
                    score = self._calculate_product_score(product, query)
                    
                    # Crear referencia
                    from src.core.data.product_reference import ProductReference
                    ref = ProductReference.from_product(
                        product=product,
                        score=score,
                        source="rag"
                    )
                    product_references.append(ref)
                    
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Error procesando resultado: {e}")
                    continue
            
            # üî• Ordenar solo si hay referencias
            if product_references:
                product_references.sort(key=lambda x: x.score, reverse=True)
            
            return product_references
            
        except Exception as e:
            self.logger.error(f"‚ùå Error en b√∫squeda sem√°ntica: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return []
    
    def _calculate_product_score(self, product: Any, query: str) -> float:
        """Calcula un score simple para el producto basado en la query."""
        try:
            # üî• Asegurar que product tenga atributos necesarios
            if not product or not hasattr(product, 'title'):
                return 0.1
            
            # M√©todo simple: similitud de texto
            from difflib import SequenceMatcher
            
            # üî• Asegurar que title no sea None
            title = getattr(product, 'title', '') or ''
            
            text_sim = SequenceMatcher(None, query.lower(), title.lower()).ratio()
            
            # üî• Agregar factores adicionales con manejo seguro de None
            price_factor = 0.5  # Valor por defecto
            if hasattr(product, 'price') and product.price is not None:
                price = float(product.price) if product.price else 0.0
                # Productos con precio definido obtienen mejor score
                price_factor = 0.8 if price > 0 else 0.3
            
            rating_factor = 0.5  # Valor por defecto
            if hasattr(product, 'average_rating') and product.average_rating is not None:
                rating = float(product.average_rating) if product.average_rating else 0.0
                rating_factor = min(1.0, rating / 5.0)
            
            # Combinar scores (60% similitud, 20% precio, 20% rating)
            final_score = (text_sim * 0.6) + (price_factor * 0.2) + (rating_factor * 0.2)
            
            # üî• Asegurar que el score est√© en rango [0, 1]
            return max(0.0, min(1.0, final_score))
            
        except Exception as e:
            self.logger.warning(f"Error calculando score: {e}")
            return 0.1  # Score m√≠nimo
    
    def _enhance_with_ml(self, 
                        results: List[ProductReference], 
                        query: str) -> List[ProductReference]:
        """
        Enriquece resultados con procesamiento ML.
        Usa settings como √∫nica fuente de verdad para configuraci√≥n ML.
        """
        if not results or not self.config.ml_enabled:
            return results
        
        enhanced_results = []
        query_embedding = self._get_query_embedding(query)
        
        for ref in results:
            # Solo procesar si el producto no tiene ya ML features
            if not ref.is_ml_processed:
                enhanced_ref = self._apply_ml_to_reference(ref, query_embedding)
                enhanced_results.append(enhanced_ref)
            else:
                # Si ya tiene ML, calcular similitud adicional
                if query_embedding is not None and ref.has_embedding:
                    # üî• CORRECCI√ìN: Asegurar que ref.embedding no sea None
                    if ref.embedding is not None:
                        similarity = self._calculate_similarity_with_none_check(
                            query_embedding, 
                            ref.embedding
                        )
                        ref.update_ml_features({
                            'query_similarity': similarity,
                            'ml_enhanced': True
                        })
                enhanced_results.append(ref)
        
        # Ordenar por puntaje ML mejorado
        if self.config.use_ml_embeddings and query_embedding:
            enhanced_results.sort(
                key=lambda x: self._calculate_ml_score(x, query_embedding),
                reverse=True
            )
        
        self.logger.debug(f"ü§ñ ML Enhancement aplicado a {len(enhanced_results)} productos")
        return enhanced_results
    
    def _apply_ml_to_reference(self, 
                              ref: ProductReference,
                              query_embedding: Optional[List[float]] = None) -> ProductReference:
        """Aplica procesamiento ML a un ProductReference."""
        if not ref.product:
            return ref
        
        ml_data: Dict[str, Any] = {}
        
        # Extraer caracter√≠sticas ML seg√∫n configuraci√≥n
        if 'category' in self.config.ml_features:
            category = self._predict_category(ref.product)
            if category:
                ml_data['predicted_category'] = category
                ml_data['category_confidence'] = 0.8  # Valor por defecto
        
        if 'entities' in self.config.ml_features:
            entities = self._extract_entities(ref.product)
            if entities:
                ml_data['extracted_entities'] = entities
        
        if 'embedding' in self.config.ml_features and self.embedding_model:
            # Generar embedding si no existe
            if not ref.has_embedding:
                text = ref.title if hasattr(ref, 'title') else ""
                embedding = self.embedding_model.encode(text)
                # üî• CORRECCI√ìN: Convertir embedding a List[float] de forma segura
                embedding_list = self._convert_to_float_list(embedding)
                ml_data['embedding'] = embedding_list
                ml_data['embedding_model'] = settings.ML_EMBEDDING_MODEL
            
            # Calcular similitud con query si hay embedding
            if query_embedding is not None and 'embedding' in ml_data:
                embedding2 = ml_data['embedding']
                if isinstance(embedding2, list):
                    similarity = self._calculate_similarity_with_none_check(
                        query_embedding, 
                        embedding2
                    )
                    ml_data['similarity_score'] = similarity
        
        if 'tags' in self.config.ml_features:
            tags = self._generate_tags(ref.product)
            if tags:
                ml_data['ml_tags'] = tags
        
        # üî• Crear referencia mejorada con ML
        if ml_data:
            ml_score = ml_data.get('similarity_score', 0.0) or ml_data.get('category_confidence', 0.0)
            
            # Usar la funci√≥n de conveniencia de product_reference
            enhanced_ref = create_ml_enhanced_reference(
                product=ref.product,
                ml_score=ml_score,
                ml_data=ml_data
            )
            
            # Preservar score original
            enhanced_ref.score = ref.score
            
            return enhanced_ref
        
        return ref
    
    def _predict_category(self, product: Product) -> Optional[str]:
        """Predice categor√≠a usando configuraci√≥n del sistema."""
        if not product or not product.title:
            return None
        
        text = f"{product.title} {product.description or ''}".lower()
        
        # Buscar coincidencias con categor√≠as del sistema
        for category in settings.ML_CATEGORIES:
            if category.lower() in text:
                return category
        
        # Si no encuentra, usar categor√≠a principal si existe
        return product.main_category
    
    def _extract_entities(self, product: Product) -> Dict[str, List[str]]:
        """Extrae entidades del producto."""
        entities = {
            "PRODUCT": [],
            "BRAND": [],
            "CATEGORY": []
        }
        
        text = f"{product.title} {product.description or ''}"
        
        # Extracci√≥n simple de entidades
        import re
        # Patr√≥n para marcas (palabras con may√∫scula)
        brand_pattern = r'\b[A-Z][a-z]+\b'
        brands = re.findall(brand_pattern, text)
        entities["BRAND"] = list(set(brands))[:5]
        
        # Palabras clave de producto
        product_keywords = ['pro', 'max', 'plus', 'mini', 'ultra', 'lite']
        words = text.lower().split()
        for word in words:
            if len(word) > 3 and word not in ['this', 'that', 'with', 'from']:
                entities["PRODUCT"].append(word)
        
        entities["PRODUCT"] = list(set(entities["PRODUCT"]))[:10]
        
        return entities
    
    def _generate_tags(self, product: Product) -> List[str]:
        """Genera tags autom√°ticos para el producto."""
        tags = []
        
        if product.title:
            # Extraer palabras clave del t√≠tulo
            import re
            words = re.findall(r'\b[a-z]{3,}\b', product.title.lower())
            tags.extend(words[:5])
        
        if product.main_category:
            tags.append(product.main_category.lower())
        
        if hasattr(product, 'ml_tags') and product.ml_tags:
            tags.extend(product.ml_tags[:3])
        
        return list(set(tags))[:8]
    
    def _get_query_embedding(self, query: str) -> Optional[List[float]]:
        """Obtiene embedding de la query."""
        if query in self._query_cache:
            return self._query_cache[query]
        
        if not self.config.use_ml_embeddings or not self.embedding_model:
            return None
        
        try:
            embedding = self.embedding_model.encode(query)
            # üî• CORRECCI√ìN: Convertir a List[float] de forma segura
            embedding_list = self._convert_to_float_list(embedding)
            self._query_cache[query] = embedding_list
            return embedding_list
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Error generando embedding para query: {e}")
            return None
    
    def _convert_to_float_list(self, embedding: Any) -> List[float]:
        """Convierte cualquier tipo de embedding a List[float]."""
        try:
            if hasattr(embedding, 'tolist'):
                # Para numpy arrays y tensores
                result = embedding.tolist()
            elif isinstance(embedding, (list, tuple, np.ndarray)):
                # Para listas, tuplas y arrays de numpy
                result = list(embedding)
            else:
                # Para otros tipos, intentar conversi√≥n directa
                result = list(embedding)
            
            # Asegurar que todos los elementos sean float
            return [float(x) for x in result]
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Error convirtiendo embedding a lista: {e}")
            # Devolver lista vac√≠a como fallback
            return []
    
    def _calculate_similarity_with_none_check(self, 
                                            embedding1: Optional[Sequence[float]], 
                                            embedding2: Optional[Sequence[float]]) -> float:
        """Calcula similitud coseno entre embeddings con chequeo de None."""
        if embedding1 is None or embedding2 is None:
            return 0.0
        
        try:
            # üî• CORRECCI√ìN: Asegurar que son listas o arrays convertibles
            v1 = np.array(embedding1, dtype=np.float32)
            v2 = np.array(embedding2, dtype=np.float32)
            
            # Normalizar vectores
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            # Calcular similitud coseno
            similarity = np.dot(v1, v2) / (norm1 * norm2)
            
            # Asegurar valor entre 0 y 1
            return max(0.0, min(1.0, similarity))
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Error calculando similitud: {e}")
            return 0.0
    
    # üî• MANTENER m√©todo original para compatibilidad
    def _calculate_similarity(self, 
                             embedding1: List[float], 
                             embedding2: List[float]) -> float:
        """Calcula similitud coseno entre embeddings (m√©todo original)."""
        return self._calculate_similarity_with_none_check(embedding1, embedding2)
    
    def _calculate_ml_score(self, 
                           ref: ProductReference, 
                           query_embedding: List[float]) -> float:
        """Calcula puntaje ML combinado para un producto."""
        if not ref.is_ml_processed:
            return ref.score
        
        base_score = ref.score
        ml_bonus = 0.0
        
        # Bonificaci√≥n por similitud ML
        similarity = ref.ml_features.get('similarity_score')
        if similarity:
            ml_bonus += similarity * self.config.ml_embedding_weight
        
        # Bonificaci√≥n por categor√≠a predicha
        if 'predicted_category' in ref.ml_features:
            ml_bonus += 0.1 * self.config.ml_embedding_weight
        
        # Combinar scores
        return base_score * (1 - self.config.ml_embedding_weight) + ml_bonus
    
    def _rerank_results(self, 
                       results: List[ProductReference], 
                       query: str, 
                       user_id: Optional[str] = None) -> List[ProductReference]:
        """Aplica re-ranking a los resultados con RLHF y Collaborative Filter."""
        if not results:
            return []
        
        reranked = []
        
        # üî• INICIALIZAR collab_score aqu√≠ para evitar "possibly unbound"
        collab_score = 0.0
        
        for ref in results[:self.config.max_retrieved]:
            base_score = ref.score
            
            # üî• Aplicar RLHF scoring si disponible
            rlhf_score = 0.0
            if self.rlhf_model:
                # Usar t√≠tulo como texto para RLHF
                text = ref.title if hasattr(ref, 'title') else ""
                rlhf_score = self._score_with_rlhf(query, text)
            
            # üî• Aplicar Collaborative Filter si hay usuario
            collab_score = 0.0  # üî• INICIALIZAR en cada iteraci√≥n
            if user_id and self._collaborative_filter:
                # Convertir ref a string (product id) para collaborative filter
                product_id_str = str(ref.id) if hasattr(ref, 'id') and ref.id is not None else ""
                collab_scores = self._collaborative_filter.get_collaborative_scores(
                    user_id, 
                    product_id_str
                )
                # Obtener score para este producto espec√≠fico
                if collab_scores and hasattr(ref, 'id'):
                    collab_score = collab_scores.get(str(ref.id), 0.0)
            
            # üî• Combinar scores (60% base, 20% RLHF, 20% Collaborative)
            final_score = (
                base_score * 0.6 +
                rlhf_score * 0.2 +
                collab_score * 0.2
            )
            
            # Crear copia con nuevo score
            new_ref = ProductReference(
                id=ref.id,
                product=ref.product,
                score=final_score,
                source=ref.source,
                confidence=ref.confidence,
                metadata=ref.metadata.copy(),
                ml_features=ref.ml_features.copy()
            )
            reranked.append(new_ref)
        
        # Ordenar
        reranked.sort(key=lambda x: x.score, reverse=True)
        final_results = reranked[:self.config.max_final]
        
        logger.info(f"üîÑ Re-ranking aplicado: RLHF={self.rlhf_model is not None}, CF={collab_score>0}")


        if self.config.ml_enabled: # O una flag especifica use_rl
            self.logger.info("üé≤ Aplicando pol√≠tica RL (Epsilon-Greedy)")
            final_results = self.rl_bandit.rank(reranked)
        else:
            final_results = reranked

        # Asegurar l√≠mite final
        return final_results[:self.config.max_final]
    
    # üî• NUEVO: M√©todo para usar RLHF en scoring
    def _apply_rlhf_scoring(self, query: str, references: List[ProductReference]) -> Dict[str, float]:
        """Aplica scoring RLHF a las referencias"""
        if not self.rlhf_model or not references:
            return {}
        
        scores = {}
        try:
            for ref in references:
                # Usar t√≠tulo como texto
                text = ref.title if hasattr(ref, 'title') else ""
                
                # Puntuar con modelo RLHF
                score = self._score_with_rlhf(query, text)
                scores[ref.id] = score
            
            logger.debug(f"RLHF scoring aplicado a {len(scores)} productos")
            return scores
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error en RLHF scoring: {e}")
            return {}
    
    def _score_with_rlhf(self, query: str, response: str) -> float:
        """Usa modelo RLHF para puntuar respuesta"""
        try:
            if not self.rlhf_model:
                return 0.5  # Score neutral
            
            # Tokenizar
            inputs = self.rlhf_model.tokenizer(
                f"Query: {query} Response: {response}",
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=256
            ).to(self.rlhf_model.device)
            
            # Predecir
            with torch.no_grad():
                outputs = self.rlhf_model.model(**inputs)
                score = torch.sigmoid(outputs.logits).item()
            
            return max(0.0, min(1.0, score))
            
        except Exception:
            return 0.5
    
    def _calculate_rerank_score(self, 
                               ref: ProductReference, 
                               query: str, 
                               user_id: Optional[str] = None) -> float:
        """Calcula score de re-ranking combinando m√∫ltiples factores."""
        base_score = ref.score
        
        # Factor de popularidad
        popularity_score = self._calculate_popularity_score(ref)
        
        # Factor de diversidad (evitar productos similares)
        diversity_score = self._calculate_diversity_score(ref, query)
        
        # Factor de novedad
        freshness_score = self._calculate_freshness_score(ref)
        
        # Factor personalizado si hay usuario
        personalization_score = 0.0
        if user_id:
            personalization_score = self._calculate_personalization_score(ref, user_id)
        
        # Combinar scores con ponderaciones
        final_score = (
            base_score * self.config.semantic_weight +
            popularity_score * self.config.popularity_weight +
            diversity_score * self.config.diversity_weight +
            freshness_score * self.config.freshness_weight +
            personalization_score * 0.2  # Peso fijo para personalizaci√≥n
        )
        
        return final_score
    
    def _calculate_popularity_score(self, ref: ProductReference) -> float:
        """Calcula score de popularidad basado en rating."""
        # Valor por defecto si no hay producto
        if not ref or not ref.product:
            return 0.5
        
        # Valores seguros
        rating = getattr(ref.product, 'average_rating', 0.0) or 0.0
        rating_count = getattr(ref.product, 'rating_count', 0) or 0
        
        # Convertir a n√∫meros
        try:
            rating_num = float(rating)
            count_num = int(rating_count)
        except (ValueError, TypeError):
            return 0.5
        
        # L√≥gica de c√°lculo
        if count_num > 100:
            return min(1.0, rating_num / 5.0)
        elif count_num > 10:
            return (rating_num / 5.0) * 0.8
        else:
            return 0.5  # Valor neutral para pocas o ninguna review
    
    def _calculate_diversity_score(self, 
                                  ref: ProductReference, 
                                  query: str) -> float:
        """Calcula score de diversidad para evitar resultados similares."""
        # Por ahora, implementaci√≥n simple
        # En una implementaci√≥n real, se comparar√≠a con otros resultados
        return 0.7
    
    def _calculate_freshness_score(self, ref: ProductReference) -> float:
        """Calcula score de novedad/actualidad."""
        # Por ahora, implementaci√≥n simple
        return 0.8
    
    def _calculate_personalization_score(self, 
                                        ref: ProductReference, 
                                        user_id: str) -> float:
        """Calcula score de personalizaci√≥n basado en historial del usuario."""
        # Por ahora, implementaci√≥n simple
        # En una implementaci√≥n real, se consultar√≠a el historial del usuario
        return 0.6
    
    def _generate_answer(self, 
                        query: str, 
                        products: List[ProductReference]) -> str:
        """Genera respuesta usando LLM o plantilla simple."""
        # Si hay LLM disponible, usarlo
        if self.config.local_llm_enabled and self.llm_client and products:
            try:
                # Construir contexto con productos
                context = self._build_context_for_llm(products)
                
                prompt = f"""
                Eres un asistente de recomendaciones de Amazon.
                Usuario pregunta: "{query}"
                
                Productos disponibles para recomendar:
                {context}
                
                Genera una respuesta √∫til y natural que recomiende los productos m√°s relevantes.
                Incluye detalles espec√≠ficos de los productos como precio, caracter√≠sticas y por qu√© son relevantes.
                """
                
                response = self.llm_client.generate(prompt)
                return response.strip()
                
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Error generando respuesta con LLM: {e}")
        
        # Fallback a plantilla simple
        return self._generate_template_answer(query, products)
    
    def _build_context_for_llm(self, products: List[ProductReference]) -> str:
        """Construye contexto para el LLM."""
        context_lines = []
        
        for i, ref in enumerate(products[:3]):  # Limitar a 3 productos para contexto
            title = ref.title[:100]
            price = ref.price
            category = ref.ml_features.get('predicted_category') or ref.metadata.get('main_category', 'Unknown')
            
            line = f"{i+1}. {title} - ${price:.2f} - Categor√≠a: {category}"
            context_lines.append(line)
        
        return "\n".join(context_lines)
    
    def _generate_template_answer(self, query: str, products: List[ProductReference]) -> str:
        """Genera respuesta usando plantilla simple con categor√≠as mejoradas."""
        if not products:
            return f"Lo siento, no encontr√© productos para '{query}'."
        
        # Construir respuesta con plantilla
        answer_parts = [f"Encontr√© {len(products)} productos para '{query}':\n"]
        
        for i, ref in enumerate(products[:self.config.max_final]):
            title = ref.title[:80]
            price = ref.price
            
            # üî• CORRECCI√ìN: Usar el m√©todo mejorado de extracci√≥n
            category = self._extract_category_for_display(ref, title)
            
            # A√±adir emojis basados en categor√≠a
            emoji = self._get_category_emoji(category)
            
            # üî• MOSTRAR CATEGOR√çA en la respuesta
            answer_parts.append(
                f"{emoji} {i+1}. {title[:60]} "
                f"(üí∞ ${price:.2f} | üè∑Ô∏è {category})"
            )
        
        # A√±adir recomendaci√≥n final
        if len(products) > 1:
            best_product = products[0]
            best_title = best_product.title[:60]
            best_price = best_product.price
            
            best_category = self._extract_category_for_display(best_product, best_title)
            best_emoji = self._get_category_emoji(best_category)
            
            answer_parts.append(
                f"\n{best_emoji} **Recomendaci√≥n principal**: {best_title} "
                f"(üí∞ ${best_price:.2f} | üè∑Ô∏è {best_category})"
            )
        
        return "\n".join(answer_parts)
    
    def _extract_category_for_display(self, ref: ProductReference, title: str) -> str:
        """Extrae la mejor categor√≠a para mostrar de m√∫ltiples fuentes."""
        # üî• PRIMERO: Intentar extraer del t√≠tulo (m√°s confiable para Nintendo)
        if 'nintendo' in title.lower() or 'wii' in title.lower() or 'gamecube' in title.lower():
            return 'Video Games'
        
        if 'playstation' in title.lower() or 'ps4' in title.lower() or 'ps5' in title.lower():
            return 'Video Games'
        
        if 'xbox' in title.lower():
            return 'Video Games'
        
        # Luego seguir con la l√≥gica existente...
        category = 'General'
        
        # 1. Intentar de ml_features (predicci√≥n ML en tiempo real)
        if ref.ml_features and 'predicted_category' in ref.ml_features:
            category = ref.ml_features['predicted_category']
            self.logger.debug(f"[DEBUG] Usando ml_features: {category}")
        
        # 2. Intentar de metadata (guardado en √≠ndice Chroma)
        elif ref.metadata and 'main_category' in ref.metadata:
            category = ref.metadata['main_category']
            self.logger.debug(f"[DEBUG] Usando metadata['main_category']: {category}")
        
        # 3. Intentar de metadata con otro nombre de campo
        elif ref.metadata:
            # Buscar cualquier campo que contenga "categor" en el nombre
            for key in ref.metadata.keys():
                if 'categor' in key.lower():
                    category = ref.metadata[key]
                    self.logger.debug(f"[DEBUG] Usando metadata['{key}']: {category}")
                    break
        
        # 4. Si a√∫n es "General", extraer del t√≠tulo
        if category == 'General':
            extracted = self._extract_category_from_title(title)
            if extracted != 'General':
                category = extracted
                self.logger.debug(f"[DEBUG] Usando extra√≠da del t√≠tulo: {category}")
        
        self.logger.debug(f"[DEBUG] Categor√≠a final: {category}")
        return category

    def _extract_category_from_title(self, title: str) -> str:
        """Extrae categor√≠a del t√≠tulo usando palabras clave."""
        title_lower = title.lower()
        
        # Diccionario de palabras clave
        category_keywords = {
            'Video Games': ['nintendo', 'playstation', 'xbox', 'switch', 'wii', 'gamecube',
                        'ps4', 'ps5', 'xbox one', 'game', 'video game', 'videogame',
                        'switch', 'nes', 'snes', 'n64', 'gameboy', '3ds', 'ds'],
            'Electronics': ['iphone', 'samsung', 'android', 'smartphone', 'phone', 'tablet',
                        'laptop', 'computer', 'pc', 'macbook', 'electronic'],
            'Books': ['book', 'novel', 'author', 'edition', 'hardcover', 'paperback'],
            'Sports': ['wwe', 'fight', 'combat', 'sport', 'fitness', 'gym', 'ball'],
            'Toys': ['toy', 'lego', 'doll', 'action figure', 'puzzle', 'board game'],
            'Home': ['kitchen', 'home', 'furniture', 'appliance', 'cookware'],
            'Clothing': ['shirt', 't-shirt', 'pants', 'jeans', 'dress', 'jacket'],
            'Beauty': ['beauty', 'makeup', 'cosmetic', 'skincare', 'perfume'],
            'Automotive': ['car', 'auto', 'vehicle', 'tire', 'engine', 'motor'],
            'Office': ['office', 'stationery', 'pen', 'pencil', 'notebook']
        }
        
        for category, keywords in category_keywords.items():
            for keyword in keywords:
                if keyword in title_lower:
                    return category
        
        return 'General'
    
    def _get_category_emoji(self, category: str) -> str:
        """Devuelve emoji apropiado para la categor√≠a."""
        emoji_map = {
            'Video Games': 'üéÆ',
            'Electronics': 'üì±',
            'Books': 'üìö',
            'Clothing': 'üëï',
            'Home': 'üè†',
            'Sports': '‚öΩ',
            'Beauty': 'üíÑ',
            'Toys': 'üß∏',
            'Automotive': 'üöó',
            'Office': 'üíº'
        }
        
        for key, emoji in emoji_map.items():
            if key.lower() in category.lower():
                return emoji
        
        return 'üì¶'  # Emoji por defecto
    
    # --------------------------------------------------
    # M√©todos de utilidad y configuraci√≥n
    # --------------------------------------------------
    
    def get_config_summary(self) -> Dict[str, Any]:
        """Obtiene resumen de configuraci√≥n."""
        return {
            "rag_config": {
                "mode": self.config.mode.value,
                "ml_enabled": self.config.ml_enabled,
                "ml_features": self.config.ml_features,
                "local_llm_enabled": self.config.local_llm_enabled,
                "max_final_results": self.config.max_final,
                "enable_reranking": self.config.enable_reranking
            },
            "system_settings": {
                "ML_ENABLED": settings.ML_ENABLED,
                "ML_FEATURES": list(settings.ML_FEATURES),
                "LOCAL_LLM_ENABLED": settings.LOCAL_LLM_ENABLED,
                "LOCAL_LLM_MODEL": settings.LOCAL_LLM_MODEL
            },
            "components": {
                "retriever_loaded": self._retriever is not None,
                "llm_client_loaded": self._llm_client is not None,
                "embedding_model_loaded": self._embedding_model is not None,
                "rlhf_pipeline": self.rlhf_pipeline is not None,
                "collaborative_filter": self._collaborative_filter is not None
            }
        }
    
    def update_config(self, **kwargs) -> None:
        """Actualiza configuraci√≥n din√°micamente."""
        for key, value in kwargs.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
                self.logger.info(f"üì° Config actualizada: {key}={value}")
    
    def clear_cache(self) -> None:
        """Limpia cach√© interno."""
        self._query_cache.clear()
        self.logger.info("üóëÔ∏è  Cache limpiado")
    
    def test_components(self) -> Dict[str, Any]:
        """Prueba todos los componentes del sistema."""
        results = {
            "retriever": False,
            "llm_client": False,
            "embedding_model": False,
            "rlhf_pipeline": self.rlhf_pipeline is not None,
            "collaborative_filter": self._collaborative_filter is not None,
            "errors": []
        }
        
        # Probar retriever
        try:
            _ = self.retriever
            results["retriever"] = True
        except Exception as e:
            results["errors"].append(f"Retriever: {e}")
        
        # Probar LLM client
        if self.config.local_llm_enabled:
            try:
                _ = self.llm_client
                results["llm_client"] = True
            except Exception as e:
                results["errors"].append(f"LLM Client: {e}")
        
        # Probar embedding model
        if self.config.use_ml_embeddings:
            try:
                _ = self.embedding_model
                results["embedding_model"] = True
            except Exception as e:
                results["errors"].append(f"Embedding Model: {e}")
        
        return results  # ‚Üê test_components termina AQU√ç


    # üî• CORRECCI√ìN: log_feedback debe estar aqu√≠, NO dentro de test_components
    def log_feedback(self, query: str, answer: str, rating: int, user_id: Optional[str] = None) -> None:
        """Registra feedback y ENTRENA el modelo RL."""
        try:
            # 1. Convertir Rating (1-5) a Recompensa (-1 a +1)
            # 1-2 = -1 (Castigo), 3 = 0 (Neutro), 4-5 = +1 (Premio)
            reward = 0
            if rating >= 4:
                reward = 1
            elif rating <= 2:
                reward = -1
            
            self.logger.info(f"üì¢ Feedback recibido: {rating}‚≠ê -> Reward: {reward}")
            # üî• SIMPLIFICADO: Guardar feedback en archivo o base de datos local
            feedback_data = {
                "query": query,
                "answer": answer[:500],  # Limitar tama√±o
                "rating": rating,
                "user_id": user_id or "anonymous",
                "timestamp": time.time(),
                "agent_mode": self.config.mode.value
            }
            
            # Crear directorio si no existe
            feedback_dir = Path("data/feedback")
            feedback_dir.mkdir(parents=True, exist_ok=True)
            
            # Guardar en archivo JSON
            import json
            from datetime import datetime
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = feedback_dir / f"feedback_{timestamp}_{rating}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(feedback_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"üìù Feedback guardado en {filename}")
            
            # üî• Actualizar Collaborative Filter si hay usuario
            if user_id and self._collaborative_filter:
                try:
                    # üî• CORRECCI√ìN: Llamar correctamente a get_collaborative_scores
                    _ = self._collaborative_filter.get_collaborative_scores(
                        user_or_profile=user_id,
                        query_or_candidates=query
                    )
                    self.logger.debug(f"Cache de Collaborative Filter actualizado para usuario {user_id}")
                except Exception as e:
                    self.logger.debug(f"No se pudo actualizar cache de Collaborative Filter: {e}")
                
        except Exception as e:
            self.logger.error(f"‚ùå Error guardando feedback: {e}")

# ----------------------------------------------------------
# Funciones de conveniencia
# ----------------------------------------------------------

def create_rag_agent(
    mode: str = "hybrid",
    ml_enabled: Optional[bool] = None,
    local_llm_enabled: Optional[bool] = None
) -> WorkingAdvancedRAGAgent:
    """
    Crea un agente RAG con configuraci√≥n simplificada.
    
    Args:
        mode: Modo de operaci√≥n (basic, hybrid, ml_enhanced, llm_enhanced)
        ml_enabled: Habilitar ML (usa settings si es None)
        local_llm_enabled: Habilitar LLM local (usa settings si es None)
        
    Returns:
        WorkingAdvancedRAGAgent configurado
    """
    # Usar configuraci√≥n del sistema por defecto
    if ml_enabled is None:
        ml_enabled = settings.ML_ENABLED
    if local_llm_enabled is None:
        local_llm_enabled = settings.LOCAL_LLM_ENABLED
    
    # Crear configuraci√≥n
    config = RAGConfig(
        mode=RAGMode(mode),
        ml_enabled=ml_enabled,
        local_llm_enabled=local_llm_enabled
    )
    
    # Crear agente
    agent = WorkingAdvancedRAGAgent(config=config)
    
    logger.info(f"üß† RAG Agent creado en modo {mode}")
    logger.info(f"   ‚Ä¢ ML: {'‚úÖ' if ml_enabled else '‚ùå'}")
    logger.info(f"   ‚Ä¢ LLM Local: {'‚úÖ' if local_llm_enabled else '‚ùå'}")
    logger.info(f"   ‚Ä¢ RLHF: {'‚úÖ' if agent.rlhf_pipeline else '‚ùå'}")
    logger.info(f"   ‚Ä¢ Collaborative Filter: {'‚úÖ' if agent._collaborative_filter else '‚ùå'}")
    
    return agent


def test_rag_pipeline(query: str = "smartphone barato") -> Dict[str, Any]:
    """
    Prueba r√°pida del pipeline RAG.
    
    Args:
        query: Consulta de prueba
        
    Returns:
        Resultados de la prueba
    """
    logger.info(f"üß™ Probando pipeline RAG con query: '{query}'")
    
    try:
        # Crear agente
        agent = create_rag_agent(mode="hybrid")
        
        # Procesar consulta
        result = agent.process_query(query)
        
        # Preparar respuesta de prueba
        test_result = {
            "success": True,
            "query": query,
            "answer_length": len(result.get("answer", "")),
            "products_found": len(result.get("products", [])),
            "processing_time": result.get("stats", {}).get("processing_time", 0),
            "config_summary": agent.get_config_summary()
        }
        
        logger.info(f"‚úÖ Test completado: {test_result['products_found']} productos encontrados")
        return test_result
        
    except Exception as e:
        logger.error(f"‚ùå Test fall√≥: {e}")
        return {
            "success": False,
            "error": str(e),
            "query": query
        }


# ----------------------------------------------------------
# Ejecuci√≥n directa para pruebas
# ----------------------------------------------------------

if __name__ == "__main__":
    # Configurar logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("üß† WorkingAdvancedRAGAgent - Prueba directa")
    print("="*50)
    
    # Probar configuraci√≥n
    agent = create_rag_agent(mode="hybrid")
    
    # Mostrar configuraci√≥n
    config_summary = agent.get_config_summary()
    print(f"\nüìã Configuraci√≥n:")
    print(f"   ‚Ä¢ Modo: {config_summary['rag_config']['mode']}")
    print(f"   ‚Ä¢ ML: {'‚úÖ' if config_summary['rag_config']['ml_enabled'] else '‚ùå'}")
    print(f"   ‚Ä¢ LLM Local: {'‚úÖ' if config_summary['rag_config']['local_llm_enabled'] else '‚ùå'}")
    print(f"   ‚Ä¢ RLHF: {'‚úÖ' if config_summary['components']['rlhf_pipeline'] else '‚ùå'}")
    print(f"   ‚Ä¢ Collaborative Filter: {'‚úÖ' if config_summary['components']['collaborative_filter'] else '‚ùå'}")
    
    # Probar componentes
    test_results = agent.test_components()
    print(f"\nüîß Componentes:")
    print(f"   ‚Ä¢ Retriever: {'‚úÖ' if test_results['retriever'] else '‚ùå'}")
    print(f"   ‚Ä¢ LLM Client: {'‚úÖ' if test_results['llm_client'] else '‚ùå'}")
    print(f"   ‚Ä¢ Embedding Model: {'‚úÖ' if test_results['embedding_model'] else '‚ùå'}")
    print(f"   ‚Ä¢ RLHF Pipeline: {'‚úÖ' if test_results['rlhf_pipeline'] else '‚ùå'}")
    print(f"   ‚Ä¢ Collaborative Filter: {'‚úÖ' if test_results['collaborative_filter'] else '‚ùå'}")
    
    if test_results['errors']:
        print(f"\n‚ö†Ô∏è Errores encontrados:")
        for error in test_results['errors']:
            print(f"   ‚Ä¢ {error}")
    
    print("\n‚úÖ RAG Agent listo para usar")