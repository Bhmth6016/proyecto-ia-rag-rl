# src/core/rag/advanced/WorkingRAGAgent.py
"""
WorkingRAGAgent - Agente RAG avanzado con configuraciÃ³n ML centralizada.
Usa ProductReference y settings como Ãºnica fuente de verdad.
"""

import logging
from typing import List, Optional, Dict, Any, Tuple, Callable
from dataclasses import dataclass, field
from enum import Enum
import time
import torch
from pathlib import Path

# Importar configuraciÃ³n centralizada
from src.core.config import settings, get_settings
from src.core.data.product import Product
from src.core.data.product_reference import ProductReference, create_ml_enhanced_reference

logger = logging.getLogger(__name__)


class RAGMode(Enum):
    """Modos de operaciÃ³n del RAG."""
    BASIC = "basic"
    HYBRID = "hybrid"
    ML_ENHANCED = "ml_enhanced"
    LLM_ENHANCED = "llm_enhanced"


@dataclass
class RAGConfig:
    """ConfiguraciÃ³n del agente RAG."""
    # Modo de operaciÃ³n
    mode: RAGMode = RAGMode.HYBRID
    
    # ConfiguraciÃ³n de recuperaciÃ³n
    enable_reranking: bool = True
    max_retrieved: int = 15
    max_final: int = 5
    
    # ConfiguraciÃ³n ML (se hereda de settings)
    ml_enabled: bool = field(default_factory=lambda: settings.ML_ENABLED)
    ml_features: List[str] = field(default_factory=lambda: list(settings.ML_FEATURES))
    use_ml_embeddings: bool = field(default_factory=lambda: settings.ML_ENABLED and 'embedding' in settings.ML_FEATURES)
    ml_embedding_weight: float = field(default_factory=lambda: settings.ML_WEIGHT)
    
    # ConfiguraciÃ³n LLM
    local_llm_enabled: bool = field(default_factory=lambda: settings.LOCAL_LLM_ENABLED)
    local_llm_model: str = field(default_factory=lambda: settings.LOCAL_LLM_MODEL)
    use_llm_for_reranking: bool = False
    
    # ConfiguraciÃ³n de dominio
    domain: str = "general"
    use_advanced_features: bool = True
    
    # Ponderaciones para scoring hÃ­brido
    semantic_weight: float = 0.6
    popularity_weight: float = 0.2
    diversity_weight: float = 0.1
    freshness_weight: float = 0.1


class WorkingAdvancedRAGAgent:
    """
    Agente RAG avanzado que usa configuraciÃ³n ML centralizada
    y ProductReference para manejo consistente.
    """
    
    def __init__(self, config: Optional[RAGConfig] = None):
        # ğŸ”¥ Usar configuraciÃ³n centralizada
        self.settings = get_settings()
        
        # ConfiguraciÃ³n del agente
        self.config = config or RAGConfig()
        
        # Componentes del sistema (lazy loaded)
        self._retriever = None
        self._llm_client = None
        self._embedding_model = None
        
        # Cache para embeddings de queries
        self._query_cache = {}
        
        # Inicializar logger
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # ğŸ”¥ NUEVO: Pipeline RLHF
        self.rlhf_pipeline = None
        self.rlhf_model = None
        self._init_rlhf()
        
        # ğŸ”¥ NUEVO: Inicializar Collaborative Filter
        self._collaborative_filter = None
        self._init_collaborative_filter()
        
        self.logger.info(f"ğŸš€ WorkingAdvancedRAGAgent inicializado")
        self.logger.info(f"   â€¢ Modo: {self.config.mode.value}")
        self.logger.info(f"   â€¢ ML: {'âœ…' if self.config.ml_enabled else 'âŒ'}")
        self.logger.info(f"   â€¢ LLM Local: {'âœ…' if self.config.local_llm_enabled else 'âŒ'}")
        self.logger.info(f"   â€¢ RLHF: {'âœ…' if self.rlhf_pipeline else 'âŒ'}")
        self.logger.info(f"   â€¢ Collaborative Filter: {'âœ…' if self._collaborative_filter else 'âŒ'}")

    def _init_rlhf(self):
        """Inicializar componente RLHF si estÃ¡ habilitado"""
        try:
            from src.core.rag.advanced.train_pipeline import RLHFTrainingPipeline
            self.rlhf_pipeline = RLHFTrainingPipeline()
            
            # Intentar cargar modelo existente
            if (Path("data/models/rlhf_model") / "pytorch_model.bin").exists():
                self.rlhf_model = self.rlhf_pipeline.load_model()
                logger.info("ğŸ§  RLHF integrado (modelo cargado)")
            else:
                logger.info("ğŸ§  RLHF integrado (sin modelo entrenado)")
                
        except ImportError as e:
            logger.warning(f"âš ï¸ RLHF no disponible: {e}")
            self.rlhf_pipeline = None
    
    # En WorkingRAGAgent._init_collaborative_filter()
    def _init_collaborative_filter(self):
        """Inicializar Collaborative Filter si estÃ¡ habilitado"""
        try:
            from src.core.rag.advanced.collaborative_filter import CollaborativeFilter
            from src.core.data.user_manager import UserManager
            from src.core.data.product_service import ProductService  # ğŸ”¥ NUEVO
            
            # Obtener gestor de usuarios
            user_manager = UserManager()
            
            # ğŸ”¥ NUEVO: Usar ProductService real
            product_service = ProductService()
            
            # Crear filtro colaborativo con servicio real
            self._collaborative_filter = CollaborativeFilter(
                user_manager=user_manager,
                product_service=product_service,  # ğŸ”¥ Pasar servicio real
                use_ml_features=self.config.ml_enabled
            )
            
            logger.info("ğŸ¤ Collaborative Filter integrado (con ProductService)")
            
        except ImportError as e:
            logger.warning(f"âš ï¸ Collaborative Filter no disponible: {e}")
            # Fallback al servicio simple
            self._init_simple_collaborative_filter()
        except Exception as e:
            logger.warning(f"âš ï¸ Error inicializando Collaborative Filter: {e}")
    
    # --------------------------------------------------
    # Propiedades lazy
    # --------------------------------------------------
    
    @property
    def retriever(self):
        """Retriever vectorial (lazy loading)."""
        if self._retriever is None:
            try:
                from src.core.rag.basic.retriever import Retriever
                self._retriever = Retriever(
                    index_path=settings.VECTOR_INDEX_PATH,
                    embedding_model=settings.EMBEDDING_MODEL,
                    device=settings.DEVICE
                )
                self.logger.info(f"âœ… Retriever inicializado: {settings.EMBEDDING_MODEL}")
            except ImportError as e:
                self.logger.error(f"âŒ No se pudo cargar Retriever: {e}")
                raise
        return self._retriever
    
    @property
    def llm_client(self):
        """Cliente LLM local (lazy loading)."""
        if self._llm_client is None and self.config.local_llm_enabled:
            try:
                from src.core.llm.local_llm import LocalLLMClient
                self._llm_client = LocalLLMClient(
                    model=settings.LOCAL_LLM_MODEL,
                    endpoint=settings.LOCAL_LLM_ENDPOINT,
                    temperature=settings.LOCAL_LLM_TEMPERATURE,
                    timeout=settings.LOCAL_LLM_TIMEOUT
                )
                self.logger.info(f"âœ… LLM Client inicializado: {settings.LOCAL_LLM_MODEL}")
            except ImportError as e:
                self.logger.warning(f"âš ï¸ No se pudo cargar LocalLLMClient: {e}")
            except Exception as e:
                self.logger.error(f"âŒ Error inicializando LLM: {e}")
        return self._llm_client
    
    @property
    def embedding_model(self):
        """Modelo de embeddings (lazy loading)."""
        if self._embedding_model is None and self.config.use_ml_embeddings:
            try:
                from sentence_transformers import SentenceTransformer
                self._embedding_model = SentenceTransformer(settings.ML_EMBEDDING_MODEL)
                self.logger.info(f"âœ… Embedding Model cargado: {settings.ML_EMBEDDING_MODEL}")
            except ImportError as e:
                self.logger.warning(f"âš ï¸ SentenceTransformer no disponible: {e}")
            except Exception as e:
                self.logger.error(f"âŒ Error cargando embedding model: {e}")
        return self._embedding_model
    
    # --------------------------------------------------
    # MÃ©todos principales
    # --------------------------------------------------
    
    def process_query(self, query: str, user_id: str = None) -> Dict[str, Any]:
        """
        Procesa una consulta completa usando RAG avanzado.
        """
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸ” Procesando consulta: '{query[:50]}...'")
            
            # 1. BÃºsqueda semÃ¡ntica inicial
            initial_results = self._semantic_search(query)

            # ğŸ” Opcional: imprimir resultados encontrados como solicitaban
            self.logger.debug(f"Encontrados {len(initial_results)} resultados iniciales")
            for i, ref in enumerate(initial_results[:3]):
                self.logger.debug(f"{i+1}. {ref.title[:50]}... (score: {ref.score})")

            # 2. Enrich con ML si estÃ¡ habilitado
            ml_enhanced_results = (
                self._enhance_with_ml(initial_results, query)
                if self.config.ml_enabled else initial_results
            )

            # 3. Re-ranking final
            final_results = (
                self._rerank_results(ml_enhanced_results, query, user_id)
                if self.config.enable_reranking else
                ml_enhanced_results[:self.config.max_final]
            )

            # 4. GeneraciÃ³n de respuesta con LLM
            answer = self._generate_answer(query, final_results)

            # 5. MÃ©tricas
            processing_time = time.time() - start_time

            response = {
                "query": query,
                "answer": answer,
                "products": final_results,
                "stats": {
                    "processing_time": round(processing_time, 2),
                    "initial_results": len(initial_results),
                    "final_results": len(final_results),
                    "ml_enhanced": self.config.ml_enabled,
                    "reranking_enabled": self.config.enable_reranking,
                }
            }

            self.logger.info(f"âœ… Consulta procesada en {processing_time:.2f}s")
            return response

        except Exception as e:
            import traceback
            self.logger.error(f"âŒ Error procesando consulta: {e}")
            self.logger.error(traceback.format_exc())

            return {
                "query": query,
                "answer": "Lo siento, hubo un error procesando tu consulta.",
                "products": [],
                "error": str(e)
            }

    
    # REEMPLAZA el mÃ©todo _semantic_search con esta versiÃ³n corregida:
    def _semantic_search(self, query: str) -> List[ProductReference]:
        """BÃºsqueda semÃ¡ntica usando embeddings."""
        try:
            # ğŸ”¥ SIMPLIFICADO: Usar search() del retriever
            raw_results = self.retriever.search(
                query=query,
                k=self.config.max_retrieved
            )
            
            product_references = []
            for product in raw_results:
                try:
                    # ğŸ”¥ Validar que el producto sea vÃ¡lido
                    if not product or not hasattr(product, 'title'):
                        continue
                    
                    # ğŸ”¥ Asegurar que el tÃ­tulo no sea None
                    if not product.title:
                        continue
                    
                    # Calcular score
                    score = self._calculate_product_score(product, query)
                    
                    # Crear referencia
                    from src.core.data.product_reference import ProductReference
                    ref = ProductReference.from_product(
                        product=product,
                        score=score,
                        source="rag"
                    )
                    product_references.append(ref)
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Error procesando resultado: {e}")
                    continue
            
            # ğŸ”¥ Ordenar solo si hay referencias
            if product_references:
                product_references.sort(key=lambda x: x.score, reverse=True)
            
            return product_references
            
        except Exception as e:
            self.logger.error(f"âŒ Error en bÃºsqueda semÃ¡ntica: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return []

    # ğŸ”¥ AÃ‘ADE este mÃ©todo que falta:
    def _calculate_initial_score(self, product: Product, query: str) -> float:
        """Calcula score inicial basado en similitud semÃ¡ntica."""
        if not product or not query:
            return 0.0
        
        try:
            # MÃ©todo simple usando SequenceMatcher para similitud de texto
            from difflib import SequenceMatcher
            
            # Calcular similitud basada en tÃ­tulo y descripciÃ³n
            title = getattr(product, 'title', '')
            description = getattr(product, 'description', '')
            
            # Ponderar tÃ­tulo mÃ¡s que descripciÃ³n
            title_sim = SequenceMatcher(None, query.lower(), title.lower()).ratio()
            desc_sim = SequenceMatcher(None, query.lower(), description.lower()).ratio() if description else 0
            
            # Combinar scores (70% tÃ­tulo, 30% descripciÃ³n)
            score = (title_sim * 0.7) + (desc_sim * 0.3)
            
            # Ajustar con otros factores
            price_factor = self._calculate_price_factor(product)
            rating_factor = self._calculate_rating_factor(product)
            
            final_score = score * 0.6 + price_factor * 0.2 + rating_factor * 0.2
            return min(1.0, max(0.0, final_score))
            
        except Exception:
            return 0.3  # Score mÃ­nimo

    def _calculate_price_factor(self, product: Product) -> float:
        """Factor basado en precio (productos con precio definido son mejores)."""
        price = getattr(product, 'price', None)
        if price and isinstance(price, (int, float)) and price > 0:
            return 0.8  # Bueno
        return 0.3  # Malo

    def _calculate_rating_factor(self, product: Product) -> float:
        """Factor basado en rating."""
        rating = getattr(product, 'average_rating', None)
        if rating and isinstance(rating, (int, float)):
            # Normalizar a 0-1
            return min(1.0, rating / 5.0)
        return 0.5  # Neutral
    def _calculate_product_score(self, product: Any, query: str) -> float:
        """Calcula un score simple para el producto basado en la query."""
        try:
            # ğŸ”¥ Asegurar que product tenga atributos necesarios
            if not product or not hasattr(product, 'title'):
                return 0.1
            
            # MÃ©todo simple: similitud de texto
            from difflib import SequenceMatcher
            
            # ğŸ”¥ Asegurar que title no sea None
            title = getattr(product, 'title', '') or ''
            
            text_sim = SequenceMatcher(None, query.lower(), title.lower()).ratio()
            
            # ğŸ”¥ Agregar factores adicionales con manejo seguro de None
            price_factor = 0.5  # Valor por defecto
            if hasattr(product, 'price') and product.price is not None:
                price = float(product.price) if product.price else 0.0
                # Productos con precio definido obtienen mejor score
                price_factor = 0.8 if price > 0 else 0.3
            
            rating_factor = 0.5  # Valor por defecto
            if hasattr(product, 'average_rating') and product.average_rating is not None:
                rating = float(product.average_rating) if product.average_rating else 0.0
                rating_factor = min(1.0, rating / 5.0)
            
            # Combinar scores (60% similitud, 20% precio, 20% rating)
            final_score = (text_sim * 0.6) + (price_factor * 0.2) + (rating_factor * 0.2)
            
            # ğŸ”¥ Asegurar que el score estÃ© en rango [0, 1]
            return max(0.0, min(1.0, final_score))
            
        except Exception as e:
            self.logger.warning(f"Error calculando score: {e}")
            return 0.1  # Score mÃ­nimo
    def _enhance_with_ml(self, 
                        results: List[ProductReference], 
                        query: str) -> List[ProductReference]:
        """
        Enriquece resultados con procesamiento ML.
        Usa settings como Ãºnica fuente de verdad para configuraciÃ³n ML.
        """
        if not results or not self.config.ml_enabled:
            return results
        
        enhanced_results = []
        query_embedding = self._get_query_embedding(query)
        
        for ref in results:
            # Solo procesar si el producto no tiene ya ML features
            if not ref.is_ml_processed:
                enhanced_ref = self._apply_ml_to_reference(ref, query_embedding)
                enhanced_results.append(enhanced_ref)
            else:
                # Si ya tiene ML, calcular similitud adicional
                if query_embedding and ref.has_embedding:
                    similarity = self._calculate_similarity(
                        query_embedding, 
                        ref.embedding
                    )
                    ref.update_ml_features({
                        'query_similarity': similarity,
                        'ml_enhanced': True
                    })
                enhanced_results.append(ref)
        
        # Ordenar por puntaje ML mejorado
        if self.config.use_ml_embeddings and query_embedding:
            enhanced_results.sort(
                key=lambda x: self._calculate_ml_score(x, query_embedding),
                reverse=True
            )
        
        self.logger.debug(f"ğŸ¤– ML Enhancement aplicado a {len(enhanced_results)} productos")
        return enhanced_results
    
    def _apply_ml_to_reference(self, 
                              ref: ProductReference,
                              query_embedding: Optional[List[float]] = None) -> ProductReference:
        """Aplica procesamiento ML a un ProductReference."""
        if not ref.product:
            return ref
        
        ml_data = {}
        
        # Extraer caracterÃ­sticas ML segÃºn configuraciÃ³n
        if 'category' in self.config.ml_features:
            category = self._predict_category(ref.product)
            if category:
                ml_data['predicted_category'] = category
                ml_data['category_confidence'] = 0.8  # Valor por defecto
        
        if 'entities' in self.config.ml_features:
            entities = self._extract_entities(ref.product)
            if entities:
                ml_data['extracted_entities'] = entities
        
        if 'embedding' in self.config.ml_features and self.embedding_model:
            # Generar embedding si no existe
            if not ref.has_embedding:
                text = ref.product.to_text() if hasattr(ref.product, 'to_text') else ref.title
                embedding = self.embedding_model.encode(text)
                ml_data['embedding'] = embedding.tolist()
                ml_data['embedding_model'] = settings.ML_EMBEDDING_MODEL
            
            # Calcular similitud con query si hay embedding
            if query_embedding is not None and 'embedding' in ml_data:
                similarity = self._calculate_similarity(
                    query_embedding, 
                    ml_data['embedding']
                )
                ml_data['similarity_score'] = similarity
        
        if 'tags' in self.config.ml_features:
            tags = self._generate_tags(ref.product)
            if tags:
                ml_data['ml_tags'] = tags
        
        # ğŸ”¥ Crear referencia mejorada con ML
        if ml_data:
            ml_score = ml_data.get('similarity_score', 0.0) or ml_data.get('category_confidence', 0.0)
            
            # Usar la funciÃ³n de conveniencia de product_reference
            enhanced_ref = create_ml_enhanced_reference(
                product=ref.product,
                ml_score=ml_score,
                ml_data=ml_data
            )
            
            # Preservar score original
            enhanced_ref.score = ref.score
            
            return enhanced_ref
        
        return ref
    
    def _predict_category(self, product: Product) -> Optional[str]:
        """Predice categorÃ­a usando configuraciÃ³n del sistema."""
        if not product or not product.title:
            return None
        
        text = f"{product.title} {product.description or ''}".lower()
        
        # Buscar coincidencias con categorÃ­as del sistema
        for category in settings.ML_CATEGORIES:
            if category.lower() in text:
                return category
        
        # Si no encuentra, usar categorÃ­a principal si existe
        return product.main_category
    
    def _extract_entities(self, product: Product) -> Dict[str, List[str]]:
        """Extrae entidades del producto."""
        entities = {
            "PRODUCT": [],
            "BRAND": [],
            "CATEGORY": []
        }
        
        text = f"{product.title} {product.description or ''}"
        
        # ExtracciÃ³n simple de entidades
        import re
        # PatrÃ³n para marcas (palabras con mayÃºscula)
        brand_pattern = r'\b[A-Z][a-z]+\b'
        brands = re.findall(brand_pattern, text)
        entities["BRAND"] = list(set(brands))[:5]
        
        # Palabras clave de producto
        product_keywords = ['pro', 'max', 'plus', 'mini', 'ultra', 'lite']
        words = text.lower().split()
        for word in words:
            if len(word) > 3 and word not in ['this', 'that', 'with', 'from']:
                entities["PRODUCT"].append(word)
        
        entities["PRODUCT"] = list(set(entities["PRODUCT"]))[:10]
        
        return entities
    
    def _generate_tags(self, product: Product) -> List[str]:
        """Genera tags automÃ¡ticos para el producto."""
        tags = []
        
        if product.title:
            # Extraer palabras clave del tÃ­tulo
            import re
            words = re.findall(r'\b[a-z]{3,}\b', product.title.lower())
            tags.extend(words[:5])
        
        if product.main_category:
            tags.append(product.main_category.lower())
        
        if hasattr(product, 'ml_tags') and product.ml_tags:
            tags.extend(product.ml_tags[:3])
        
        return list(set(tags))[:8]
    
    def _get_query_embedding(self, query: str) -> Optional[List[float]]:
        """Obtiene embedding de la query."""
        if query in self._query_cache:
            return self._query_cache[query]
        
        if not self.config.use_ml_embeddings or not self.embedding_model:
            return None
        
        try:
            embedding = self.embedding_model.encode(query)
            self._query_cache[query] = embedding.tolist()
            return embedding.tolist()
        except Exception as e:
            self.logger.warning(f"âš ï¸ Error generando embedding para query: {e}")
            return None
    
    def _calculate_similarity(self, 
                             embedding1: List[float], 
                             embedding2: List[float]) -> float:
        """Calcula similitud coseno entre embeddings."""
        try:
            import numpy as np
            
            v1 = np.array(embedding1)
            v2 = np.array(embedding2)
            
            # Normalizar vectores
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            # Calcular similitud coseno
            similarity = np.dot(v1, v2) / (norm1 * norm2)
            
            # Asegurar valor entre 0 y 1
            return max(0.0, min(1.0, similarity))
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Error calculando similitud: {e}")
            return 0.0
    
    def _calculate_ml_score(self, 
                           ref: ProductReference, 
                           query_embedding: List[float]) -> float:
        """Calcula puntaje ML combinado para un producto."""
        if not ref.is_ml_processed:
            return ref.score
        
        base_score = ref.score
        ml_bonus = 0.0
        
        # BonificaciÃ³n por similitud ML
        similarity = ref.ml_features.get('similarity_score')
        if similarity:
            ml_bonus += similarity * self.config.ml_embedding_weight
        
        # BonificaciÃ³n por categorÃ­a predicha
        if 'predicted_category' in ref.ml_features:
            ml_bonus += 0.1 * self.config.ml_embedding_weight
        
        # Combinar scores
        return base_score * (1 - self.config.ml_embedding_weight) + ml_bonus
    
    def _rerank_results(self, 
                       results: List[ProductReference], 
                       query: str, 
                       user_id: Optional[str] = None) -> List[ProductReference]:
        """Aplica re-ranking a los resultados con RLHF y Collaborative Filter."""
        if not results:
            return []
        
        reranked = []
        
        for ref in results[:self.config.max_retrieved]:
            base_score = ref.score
            
            # ğŸ”¥ Aplicar RLHF scoring si disponible
            rlhf_score = 0.0
            if self.rlhf_model:
                text = ref.title if hasattr(ref, 'title') else ""
                rlhf_score = self._score_with_rlhf(query, text)
            
            # ğŸ”¥ Aplicar Collaborative Filter si hay usuario
            collab_score = 0.0
            if user_id and self._collaborative_filter:
                collab_scores = self._collaborative_filter.get_collaborative_scores(
                    user_id, 
                    [ref.id]
                )
                collab_score = collab_scores.get(ref.id, 0.0)
            
            # ğŸ”¥ Combinar scores (60% base, 20% RLHF, 20% Collaborative)
            final_score = (
                base_score * 0.6 +
                rlhf_score * 0.2 +
                collab_score * 0.2
            )
            
            # Crear copia con nuevo score
            new_ref = ProductReference(
                id=ref.id,
                product=ref.product,
                score=final_score,
                source=ref.source,
                confidence=ref.confidence,
                metadata=ref.metadata.copy(),
                ml_features=ref.ml_features.copy()
            )
            reranked.append(new_ref)
        
        # Ordenar
        reranked.sort(key=lambda x: x.score, reverse=True)
        final_results = reranked[:self.config.max_final]
        
        logger.info(f"ğŸ”„ Re-ranking aplicado: RLHF={self.rlhf_model is not None}, CF={collab_score>0}")
        return final_results
    
    # ğŸ”¥ NUEVO: MÃ©todo para usar RLHF en scoring
    def _apply_rlhf_scoring(self, query: str, references: List[ProductReference]) -> Dict[str, float]:
        """Aplica scoring RLHF a las referencias"""
        if not self.rlhf_model or not references:
            return {}
        
        scores = {}
        try:
            for ref in references:
                if hasattr(ref, 'text'):
                    text = ref.text
                elif hasattr(ref, 'title'):
                    text = ref.title
                else:
                    continue
                
                # Puntuar con modelo RLHF
                score = self._score_with_rlhf(query, text)
                scores[ref.id] = score
            
            logger.debug(f"RLHF scoring aplicado a {len(scores)} productos")
            return scores
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error en RLHF scoring: {e}")
            return {}
    
    def _score_with_rlhf(self, query: str, response: str) -> float:
        """Usa modelo RLHF para puntuar respuesta"""
        try:
            if not self.rlhf_model:
                return 0.5  # Score neutral
            
            # Tokenizar
            inputs = self.rlhf_model.tokenizer(
                f"Query: {query} Response: {response}",
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=256
            ).to(self.rlhf_model.device)
            
            # Predecir
            with torch.no_grad():
                outputs = self.rlhf_model.model(**inputs)
                score = torch.sigmoid(outputs.logits).item()
            
            return max(0.0, min(1.0, score))
            
        except Exception:
            return 0.5
    
    def _calculate_rerank_score(self, 
                               ref: ProductReference, 
                               query: str, 
                               user_id: Optional[str] = None) -> float:
        """Calcula score de re-ranking combinando mÃºltiples factores."""
        base_score = ref.score
        
        # Factor de popularidad
        popularity_score = self._calculate_popularity_score(ref)
        
        # Factor de diversidad (evitar productos similares)
        diversity_score = self._calculate_diversity_score(ref, query)
        
        # Factor de novedad
        freshness_score = self._calculate_freshness_score(ref)
        
        # Factor personalizado si hay usuario
        personalization_score = 0.0
        if user_id:
            personalization_score = self._calculate_personalization_score(ref, user_id)
        
        # Combinar scores con ponderaciones
        final_score = (
            base_score * self.config.semantic_weight +
            popularity_score * self.config.popularity_weight +
            diversity_score * self.config.diversity_weight +
            freshness_score * self.config.freshness_weight +
            personalization_score * 0.2  # Peso fijo para personalizaciÃ³n
        )
        
        return final_score
    
    def _calculate_popularity_score(self, ref: ProductReference) -> float:
        """Calcula score de popularidad basado en rating."""
        # Valor por defecto si no hay producto
        if not ref or not ref.product:
            return 0.5
        
        # Valores seguros
        rating = getattr(ref.product, 'average_rating', 0.0) or 0.0
        rating_count = getattr(ref.product, 'rating_count', 0) or 0
        
        # Convertir a nÃºmeros
        try:
            rating_num = float(rating)
            count_num = int(rating_count)
        except (ValueError, TypeError):
            return 0.5
        
        # LÃ³gica de cÃ¡lculo
        if count_num > 100:
            return min(1.0, rating_num / 5.0)
        elif count_num > 10:
            return (rating_num / 5.0) * 0.8
        else:
            return 0.5  # Valor neutral para pocas o ninguna review
    
    def _calculate_diversity_score(self, 
                                  ref: ProductReference, 
                                  query: str) -> float:
        """Calcula score de diversidad para evitar resultados similares."""
        # Por ahora, implementaciÃ³n simple
        # En una implementaciÃ³n real, se compararÃ­a con otros resultados
        return 0.7
    
    def _calculate_freshness_score(self, ref: ProductReference) -> float:
        """Calcula score de novedad/actualidad."""
        # Por ahora, implementaciÃ³n simple
        return 0.8
    
    def _calculate_personalization_score(self, 
                                        ref: ProductReference, 
                                        user_id: str) -> float:
        """Calcula score de personalizaciÃ³n basado en historial del usuario."""
        # Por ahora, implementaciÃ³n simple
        # En una implementaciÃ³n real, se consultarÃ­a el historial del usuario
        return 0.6
    
    def _generate_answer(self, 
                        query: str, 
                        products: List[ProductReference]) -> str:
        """Genera respuesta usando LLM o plantilla simple."""
        # Si hay LLM disponible, usarlo
        if self.config.local_llm_enabled and self.llm_client and products:
            try:
                # Construir contexto con productos
                context = self._build_context_for_llm(products)
                
                prompt = f"""
                Eres un asistente de recomendaciones de Amazon.
                Usuario pregunta: "{query}"
                
                Productos disponibles para recomendar:
                {context}
                
                Genera una respuesta Ãºtil y natural que recomiende los productos mÃ¡s relevantes.
                Incluye detalles especÃ­ficos de los productos como precio, caracterÃ­sticas y por quÃ© son relevantes.
                """
                
                response = self.llm_client.generate(prompt)
                return response.strip()
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ Error generando respuesta con LLM: {e}")
        
        # Fallback a plantilla simple
        return self._generate_template_answer(query, products)
    
    def _build_context_for_llm(self, products: List[ProductReference]) -> str:
        """Construye contexto para el LLM."""
        context_lines = []
        
        for i, ref in enumerate(products[:3]):  # Limitar a 3 productos para contexto
            title = ref.title[:100]
            price = ref.price
            category = ref.ml_features.get('predicted_category') or ref.metadata.get('main_category', 'Unknown')
            
            line = f"{i+1}. {title} - ${price:.2f} - CategorÃ­a: {category}"
            context_lines.append(line)
        
        return "\n".join(context_lines)
    
    def _generate_template_answer(self, query: str, products: List[ProductReference]) -> str:
        """Genera respuesta usando plantilla simple con categorÃ­as mejoradas."""
        if not products:
            return f"Lo siento, no encontrÃ© productos para '{query}'."
        
        # Construir respuesta con plantilla
        answer_parts = [f"EncontrÃ© {len(products)} productos para '{query}':\n"]
        
        for i, ref in enumerate(products[:self.config.max_final]):
            title = ref.title[:80]
            price = ref.price
            
            # ğŸ”¥ CORRECCIÃ“N: Usar el mÃ©todo mejorado de extracciÃ³n
            category = self._extract_category_for_display(ref, title)
            
            # AÃ±adir emojis basados en categorÃ­a
            emoji = self._get_category_emoji(category)
            
            # ğŸ”¥ MOSTRAR CATEGORÃA en la respuesta
            answer_parts.append(
                f"{emoji} {i+1}. {title[:60]} "
                f"(ğŸ’° ${price:.2f} | ğŸ·ï¸ {category})"  # â† Â¡AHORA MUESTRA CATEGORÃA!
            )
        
        # AÃ±adir recomendaciÃ³n final
        if len(products) > 1:
            best_product = products[0]
            best_title = best_product.title[:60]
            best_price = best_product.price
            
            best_category = self._extract_category_for_display(best_product, best_title)
            best_emoji = self._get_category_emoji(best_category)
            
            answer_parts.append(
                f"\n{best_emoji} **RecomendaciÃ³n principal**: {best_title} "
                f"(ğŸ’° ${best_price:.2f} | ğŸ·ï¸ {best_category})"
            )
        
        return "\n".join(answer_parts)
    def _extract_category_for_display(self, ref: ProductReference, title: str) -> str:
        """Extrae la mejor categorÃ­a para mostrar de mÃºltiples fuentes."""
        # ğŸ”¥ PRIMERO: Intentar extraer del tÃ­tulo (mÃ¡s confiable para Nintendo)
        if 'nintendo' in title.lower() or 'wii' in title.lower() or 'gamecube' in title.lower():
            return 'Video Games'
        
        if 'playstation' in title.lower() or 'ps4' in title.lower() or 'ps5' in title.lower():
            return 'Video Games'
        
        if 'xbox' in title.lower():
            return 'Video Games'
        
        # Luego seguir con la lÃ³gica existente...
        category = 'General'
        
        # 1. Intentar de ml_features (predicciÃ³n ML en tiempo real)
        if ref.ml_features and 'predicted_category' in ref.ml_features:
            category = ref.ml_features['predicted_category']
            self.logger.debug(f"[DEBUG] Usando ml_features: {category}")
        
        # 2. Intentar de metadata (guardado en Ã­ndice Chroma)
        elif ref.metadata and 'main_category' in ref.metadata:
            category = ref.metadata['main_category']
            self.logger.debug(f"[DEBUG] Usando metadata['main_category']: {category}")
        
        # 3. Intentar de metadata con otro nombre de campo
        elif ref.metadata:
            # Buscar cualquier campo que contenga "categor" en el nombre
            for key in ref.metadata.keys():
                if 'categor' in key.lower():
                    category = ref.metadata[key]
                    self.logger.debug(f"[DEBUG] Usando metadata['{key}']: {category}")
                    break
        
        # 4. Si aÃºn es "General", extraer del tÃ­tulo
        if category == 'General':
            extracted = self._extract_category_from_title(title)
            if extracted != 'General':
                category = extracted
                self.logger.debug(f"[DEBUG] Usando extraÃ­da del tÃ­tulo: {category}")
        
        self.logger.debug(f"[DEBUG] CategorÃ­a final: {category}")
        return category

    def _extract_category_from_title(self, title: str) -> str:
        """Extrae categorÃ­a del tÃ­tulo usando palabras clave."""
        title_lower = title.lower()
        
        # Diccionario de palabras clave
        category_keywords = {
            'Video Games': ['nintendo', 'playstation', 'xbox', 'switch', 'wii', 'gamecube',
                        'ps4', 'ps5', 'xbox one', 'game', 'video game', 'videogame',
                        'switch', 'nes', 'snes', 'n64', 'gameboy', '3ds', 'ds'],
            'Electronics': ['iphone', 'samsung', 'android', 'smartphone', 'phone', 'tablet',
                        'laptop', 'computer', 'pc', 'macbook', 'electronic'],
            'Books': ['book', 'novel', 'author', 'edition', 'hardcover', 'paperback'],
            'Sports': ['wwe', 'fight', 'combat', 'sport', 'fitness', 'gym', 'ball'],
            'Toys': ['toy', 'lego', 'doll', 'action figure', 'puzzle', 'board game'],
            'Home': ['kitchen', 'home', 'furniture', 'appliance', 'cookware'],
            'Clothing': ['shirt', 't-shirt', 'pants', 'jeans', 'dress', 'jacket'],
            'Beauty': ['beauty', 'makeup', 'cosmetic', 'skincare', 'perfume'],
            'Automotive': ['car', 'auto', 'vehicle', 'tire', 'engine', 'motor'],
            'Office': ['office', 'stationery', 'pen', 'pencil', 'notebook']
        }
        
        for category, keywords in category_keywords.items():
            for keyword in keywords:
                if keyword in title_lower:
                    return category
        
        return 'General'
    def _get_category_emoji(self, category: str) -> str:
        """Devuelve emoji apropiado para la categorÃ­a."""
        emoji_map = {
            'Electronics': 'ğŸ“±',
            'Books': 'ğŸ“š',
            'Clothing': 'ğŸ‘•',
            'Home': 'ğŸ ',
            'Sports': 'âš½',
            'Beauty': 'ğŸ’„',
            'Toys': 'ğŸ§¸',
            'Automotive': 'ğŸš—',
            'Office': 'ğŸ’¼'
        }
        
        for key, emoji in emoji_map.items():
            if key.lower() in category.lower():
                return emoji
        
        return 'ğŸ“¦'  # Emoji por defecto
    
    # --------------------------------------------------
    # MÃ©todos de utilidad y configuraciÃ³n
    # --------------------------------------------------
    
    def get_config_summary(self) -> Dict[str, Any]:
        """Obtiene resumen de configuraciÃ³n."""
        return {
            "rag_config": {
                "mode": self.config.mode.value,
                "ml_enabled": self.config.ml_enabled,
                "ml_features": self.config.ml_features,
                "local_llm_enabled": self.config.local_llm_enabled,
                "max_final_results": self.config.max_final,
                "enable_reranking": self.config.enable_reranking
            },
            "system_settings": {
                "ML_ENABLED": settings.ML_ENABLED,
                "ML_FEATURES": list(settings.ML_FEATURES),
                "LOCAL_LLM_ENABLED": settings.LOCAL_LLM_ENABLED,
                "LOCAL_LLM_MODEL": settings.LOCAL_LLM_MODEL
            },
            "components": {
                "retriever_loaded": self._retriever is not None,
                "llm_client_loaded": self._llm_client is not None,
                "embedding_model_loaded": self._embedding_model is not None,
                "rlhf_pipeline": self.rlhf_pipeline is not None,
                "collaborative_filter": self._collaborative_filter is not None
            }
        }
    
    def update_config(self, **kwargs) -> None:
        """Actualiza configuraciÃ³n dinÃ¡micamente."""
        for key, value in kwargs.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
                self.logger.info(f"ğŸ“¡ Config actualizada: {key}={value}")
    
    def clear_cache(self) -> None:
        """Limpia cachÃ© interno."""
        self._query_cache.clear()
        self.logger.info("ğŸ—‘ï¸  Cache limpiado")
    
    def test_components(self) -> Dict[str, Any]:
        """Prueba todos los componentes del sistema."""
        results = {
            "retriever": False,
            "llm_client": False,
            "embedding_model": False,
            "rlhf_pipeline": self.rlhf_pipeline is not None,
            "collaborative_filter": self._collaborative_filter is not None,
            "errors": []
        }
        
        # Probar retriever
        try:
            _ = self.retriever
            results["retriever"] = True
        except Exception as e:
            results["errors"].append(f"Retriever: {e}")
        
        # Probar LLM client
        if self.config.local_llm_enabled:
            try:
                _ = self.llm_client
                results["llm_client"] = True
            except Exception as e:
                results["errors"].append(f"LLM Client: {e}")
        
        # Probar embedding model
        if self.config.use_ml_embeddings:
            try:
                _ = self.embedding_model
                results["embedding_model"] = True
            except Exception as e:
                results["errors"].append(f"Embedding Model: {e}")
        
        return results


# ----------------------------------------------------------
# Funciones de conveniencia
# ----------------------------------------------------------

def create_rag_agent(
    mode: str = "hybrid",
    ml_enabled: Optional[bool] = None,
    local_llm_enabled: Optional[bool] = None
) -> WorkingAdvancedRAGAgent:
    """
    Crea un agente RAG con configuraciÃ³n simplificada.
    
    Args:
        mode: Modo de operaciÃ³n (basic, hybrid, ml_enhanced, llm_enhanced)
        ml_enabled: Habilitar ML (usa settings si es None)
        local_llm_enabled: Habilitar LLM local (usa settings si es None)
        
    Returns:
        WorkingAdvancedRAGAgent configurado
    """
    # Usar configuraciÃ³n del sistema por defecto
    if ml_enabled is None:
        ml_enabled = settings.ML_ENABLED
    if local_llm_enabled is None:
        local_llm_enabled = settings.LOCAL_LLM_ENABLED
    
    # Crear configuraciÃ³n
    config = RAGConfig(
        mode=RAGMode(mode),
        ml_enabled=ml_enabled,
        local_llm_enabled=local_llm_enabled
    )
    
    # Crear agente
    agent = WorkingAdvancedRAGAgent(config=config)
    
    logger.info(f"ğŸ§  RAG Agent creado en modo {mode}")
    logger.info(f"   â€¢ ML: {'âœ…' if ml_enabled else 'âŒ'}")
    logger.info(f"   â€¢ LLM Local: {'âœ…' if local_llm_enabled else 'âŒ'}")
    logger.info(f"   â€¢ RLHF: {'âœ…' if agent.rlhf_pipeline else 'âŒ'}")
    logger.info(f"   â€¢ Collaborative Filter: {'âœ…' if agent._collaborative_filter else 'âŒ'}")
    
    return agent


def test_rag_pipeline(query: str = "smartphone barato") -> Dict[str, Any]:
    """
    Prueba rÃ¡pida del pipeline RAG.
    
    Args:
        query: Consulta de prueba
        
    Returns:
        Resultados de la prueba
    """
    logger.info(f"ğŸ§ª Probando pipeline RAG con query: '{query}'")
    
    try:
        # Crear agente
        agent = create_rag_agent(mode="hybrid")
        
        # Procesar consulta
        result = agent.process_query(query)
        
        # Preparar respuesta de prueba
        test_result = {
            "success": True,
            "query": query,
            "answer_length": len(result.get("answer", "")),
            "products_found": len(result.get("products", [])),
            "processing_time": result.get("stats", {}).get("processing_time", 0),
            "config_summary": agent.get_config_summary()
        }
        
        logger.info(f"âœ… Test completado: {test_result['products_found']} productos encontrados")
        return test_result
        
    except Exception as e:
        logger.error(f"âŒ Test fallÃ³: {e}")
        return {
            "success": False,
            "error": str(e),
            "query": query
        }


# ----------------------------------------------------------
# EjecuciÃ³n directa para pruebas
# ----------------------------------------------------------

if __name__ == "__main__":
    # Configurar logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("ğŸ§  WorkingAdvancedRAGAgent - Prueba directa")
    print("="*50)
    
    # Probar configuraciÃ³n
    agent = create_rag_agent(mode="hybrid")
    
    # Mostrar configuraciÃ³n
    config_summary = agent.get_config_summary()
    print(f"\nğŸ“‹ ConfiguraciÃ³n:")
    print(f"   â€¢ Modo: {config_summary['rag_config']['mode']}")
    print(f"   â€¢ ML: {'âœ…' if config_summary['rag_config']['ml_enabled'] else 'âŒ'}")
    print(f"   â€¢ LLM Local: {'âœ…' if config_summary['rag_config']['local_llm_enabled'] else 'âŒ'}")
    print(f"   â€¢ RLHF: {'âœ…' if config_summary['components']['rlhf_pipeline'] else 'âŒ'}")
    print(f"   â€¢ Collaborative Filter: {'âœ…' if config_summary['components']['collaborative_filter'] else 'âŒ'}")
    
    # Probar componentes
    test_results = agent.test_components()
    print(f"\nğŸ”§ Componentes:")
    print(f"   â€¢ Retriever: {'âœ…' if test_results['retriever'] else 'âŒ'}")
    print(f"   â€¢ LLM Client: {'âœ…' if test_results['llm_client'] else 'âŒ'}")
    print(f"   â€¢ Embedding Model: {'âœ…' if test_results['embedding_model'] else 'âŒ'}")
    print(f"   â€¢ RLHF Pipeline: {'âœ…' if test_results['rlhf_pipeline'] else 'âŒ'}")
    print(f"   â€¢ Collaborative Filter: {'âœ…' if test_results['collaborative_filter'] else 'âŒ'}")
    
    if test_results['errors']:
        print(f"\nâš ï¸ Errores encontrados:")
        for error in test_results['errors']:
            print(f"   â€¢ {error}")
    
    print("\nâœ… RAG Agent listo para usar")