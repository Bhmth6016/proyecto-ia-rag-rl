# scripts/check_ollama.py - Verifica y ayuda a configurar Ollama

import requests
import subprocess
import sys
import os
from pathlib import Path

def check_ollama():
    """Verifica el estado de Ollama y ayuda a configurarlo."""
    
    print("üîç Verificando configuraci√≥n de Ollama...")
    print("=" * 60)
    
    # Verificar si Ollama est√° instalado
    try:
        result = subprocess.run(['ollama', '--version'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            print(f"‚úÖ Ollama instalado: {result.stdout.strip()}")
        else:
            print("‚ùå Ollama no parece estar instalado o no est√° en PATH")
            print("\nüí° Para instalar Ollama:")
            print("   1. Visita https://ollama.ai/")
            print("   2. Descarga e instala Ollama")
            print("   3. Aseg√∫rate de que 'ollama' est√© en tu PATH")
            return False
    except FileNotFoundError:
        print("‚ùå Ollama no encontrado. Por favor inst√°lalo desde https://ollama.ai/")
        return False
    
    # Verificar si el servicio Ollama est√° corriendo
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Servicio Ollama est√° corriendo en http://localhost:11434")
        else:
            print(f"‚ö†Ô∏è  Ollama responde con c√≥digo {response.status_code}")
            print("üí° Intenta: ollama serve (en otra terminal)")
            return False
    except requests.ConnectionError:
        print("‚ùå No se puede conectar a Ollama en http://localhost:11434")
        print("\nüí° Soluciones:")
        print("   1. Aseg√∫rate de que Ollama est√© corriendo: ollama serve")
        print("   2. Verifica que el puerto 11434 no est√© bloqueado")
        return False
    
    # Verificar modelos disponibles
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            if models:
                print(f"‚úÖ Modelos disponibles ({len(models)}):")
                for model in models:
                    name = model.get('name', 'Unknown')
                    size = model.get('size', 0) / (1024**3)  # Convertir a GB
                    print(f"   ‚Ä¢ {name} ({size:.1f}GB)")
            else:
                print("‚ö†Ô∏è  No hay modelos descargados")
                print("\nüí° Para descargar un modelo:")
                print("   ollama pull llama3.2:3b  # Modelo peque√±o y r√°pido")
                print("   ollama pull llama3.2:1b  # Modelo muy peque√±o")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error verificando modelos: {e}")
    
    print("\nüéØ Configuraci√≥n recomendada para este proyecto:")
    print("   ‚Ä¢ Modelo: llama-3.2-3b-instruct (equilibrado)")
    print("   ‚Ä¢ Endpoint: http://localhost:11434")
    print("   ‚Ä¢ Temperature: 0.1 (respuestas m√°s deterministas)")
    
    return True

def setup_ollama_model(model_name="llama-3.2-3b-instruct"):
    """Intenta descargar el modelo si no est√° disponible."""
    
    print(f"\nüì• Configurando modelo: {model_name}")
    
    # Verificar si ya est√° descargado
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [m.get('name') for m in models]
            
            if model_name in model_names:
                print(f"‚úÖ Modelo {model_name} ya est√° descargado")
                return True
    except:
        pass
    
    # Preguntar si descargar
    print(f"‚ö†Ô∏è  El modelo {model_name} no est√° descargado")
    response = input(f"¬øDescargar {model_name}? (s/n): ").strip().lower()
    
    if response == 's':
        try:
            print("‚è≥ Descargando modelo... Esto puede tomar unos minutos.")
            print("   (Depende de tu conexi√≥n a internet)")
            
            result = subprocess.run(['ollama', 'pull', model_name], 
                                  capture_output=True, text=True)
            
            if result.returncode == 0:
                print(f"‚úÖ Modelo {model_name} descargado exitosamente")
                return True
            else:
                print(f"‚ùå Error descargando modelo: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"‚ùå Error: {e}")
            return False
    else:
        print("‚ö†Ô∏è  Saltando descarga del modelo")
        return False

if __name__ == "__main__":
    print("ü¶ô Configurador de Ollama para RAG E-commerce")
    print("=" * 60)
    
    # Verificar Ollama
    if check_ollama():
        # Configurar modelo
        setup_ollama_model("llama-3.2-3b-instruct")
        
        print("\nüéâ Configuraci√≥n completada!")
        print("\nüí° Ahora puedes ejecutar:")
        print("   python main.py rag --mode enhanced --ml")
        print("\n‚ö†Ô∏è  Si prefieres no usar LLM, ejecuta sin --ml")
        print("   python main.py rag --mode basic")
    else:
        print("\n‚ùå Ollama no est√° configurado correctamente.")
        print("üí° Para usar el sistema sin LLM:")
        print("   python main.py rag --mode basic")
        print("   python main.py rag --mode enhanced --no-ml")