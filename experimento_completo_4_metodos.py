# experimento_completo_4_metodos.py
"""
EXPERIMENTO COMPLETO VERIFICADO - 4 M√âTODOS DE RANKING
Versi√≥n consolidada y optimizada
"""
import json
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Any
import random
import logging
from datetime import datetime
import sys
import traceback
import os

# ============================================================================
# INICIALIZACI√ìN PRINCIPAL
# ============================================================================

def setup_directories():
    """Crea todos los directorios necesarios"""
    directories = [
        'data/cache',
        'results',
        'logs',
        'data/interactions',
        'data/backups'
    ]
    
    created_dirs = []
    for dir_path in directories:
        path = Path(dir_path)
        try:
            path.mkdir(parents=True, exist_ok=True)
            created_dirs.append(str(path))
        except Exception as e:
            print(f"  ‚úó Error creando {dir_path}: {e}")
    
    return created_dirs

def setup_logging():
    """Configura logging robustamente"""
    logs_dir = Path("logs")
    logs_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = logs_dir / f"experimento_{timestamp}.log"
    
    try:
        logger = logging.getLogger()
        logger.setLevel(logging.INFO)
        
        # Remover handlers existentes
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
        
        # Formato del log
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        # Handler para archivo
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
        
        # Handler para consola
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
        
        app_logger = logging.getLogger(__name__)
        app_logger.info(f"üìù Log iniciado: {log_file}")
        
        return app_logger
    except Exception as e:
        print(f"‚ö†Ô∏è  Error configurando logging: {e}")
        print(f"‚ö†Ô∏è  Usando logging b√°sico...")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            stream=sys.stdout
        )
        logger = logging.getLogger(__name__)
        logger.warning(f"Logging alternativo activado")
        return logger

def check_dependencies():
    """Verifica que las dependencias est√©n instaladas"""
    required = ['numpy', 'pandas', 'faiss']
    optional = ['scipy', 'tqdm', 'sentence_transformers']
    missing_required = []
    missing_optional = []
    
    print("üîç Verificando dependencias...")
    
    # Verificar requeridos
    for package in required:
        try:
            __import__(package)
            print(f"  ‚úì {package}")
        except ImportError:
            missing_required.append(package)
            print(f"  ‚úó {package} (REQUERIDO)")
    
    # Verificar opcionales
    for package in optional:
        try:
            __import__(package)
            print(f"  ‚úì {package} (opcional)")
        except ImportError:
            missing_optional.append(package)
            print(f"  ‚ö†Ô∏è  {package} (opcional, faltante)")
    
    if missing_required:
        print(f"\n‚ùå Paquetes REQUERIDOS faltantes: {', '.join(missing_required)}")
        print(f"üí° Ejecutar: pip install {' '.join(missing_required)}")
        return False
    
    if missing_optional:
        print(f"\n‚ö†Ô∏è  Paquetes opcionales faltantes: {', '.join(missing_optional)}")
        print(f"üí° Para funcionalidad completa: pip install scipy tqdm sentence-transformers")
    
    return True

# ============================================================================
# EJECUCI√ìN INICIAL
# ============================================================================

print("\n" + "="*60)
print("üîß INICIALIZANDO EXPERIMENTO - 4 M√âTODOS")
print("="*60)

# 1. Crear directorios primero
print("\nüìÅ Creando estructura de directorios...")
dirs = setup_directories()
print(f"   ‚úÖ Directorios creados/verificados: {len(dirs)}")

# 2. Configurar logging
print("\nüìù Configurando sistema de logging...")
logger = setup_logging()

# 3. Verificar dependencias
print("\nüîç Verificando dependencias de Python...")
deps_ok = check_dependencies()

if not deps_ok:
    print("\n‚ùå No se pueden continuar sin las dependencias requeridas")
    sys.exit(1)

print("\n" + "="*60)
print("‚úÖ SISTEMA INICIALIZADO CORRECTAMENTE")
print("="*60 + "\n")

# ============================================================================
# FUNCIONES PRINCIPALES DEL EXPERIMENTO
# ============================================================================

def load_ground_truth() -> Dict[str, List[str]]:
    """Carga ground truth REAL"""
    gt_file = Path("data/interactions/ground_truth_REAL.json")
    
    if not gt_file.exists():
        logger.error(f"‚ùå Ground truth no encontrado: {gt_file}")
        logger.info("   Ejecuta primero: python main.py interactivo")
        sys.exit(1)
    
    with open(gt_file, 'r', encoding='utf-8') as f:
        ground_truth = json.load(f)
    
    logger.info(f"üìä Ground truth cargado: {len(ground_truth)} queries")
    
    # Estad√≠sticas
    total_relevantes = sum(len(ids) for ids in ground_truth.values())
    logger.info(f"   ‚Ä¢ Productos relevantes totales: {total_relevantes}")
    
    return ground_truth

def split_train_test_stratified(ground_truth: Dict, test_size: float = 0.25, 
                               seed: int = 42) -> Tuple[List[str], List[str]]:
    """
    Split estratificado por n√∫mero de productos relevantes
    """
    random.seed(seed)
    
    # Agrupar por cantidad de relevantes
    queries_by_count = {}
    for query, ids in ground_truth.items():
        count = len(ids)
        if count not in queries_by_count:
            queries_by_count[count] = []
        queries_by_count[count].append(query)
    
    train_queries = []
    test_queries = []
    
    # Split dentro de cada grupo
    for count, queries in queries_by_count.items():
        random.shuffle(queries)
        split_idx = int(len(queries) * (1 - test_size))
        
        train_queries.extend(queries[:split_idx])
        test_queries.extend(queries[split_idx:])
    
    # Mezclar final
    random.shuffle(train_queries)
    random.shuffle(test_queries)
    
    logger.info(f"üìö Split creado: {len(train_queries)} train, {len(test_queries)} test")
    
    # Verificar que no haya overlap
    overlap = set(train_queries) & set(test_queries)
    if overlap:
        logger.warning(f"‚ö†Ô∏è  Overlap detectado: {len(overlap)} queries")
    
    return train_queries, test_queries

def load_or_create_system_v2() -> Any:
    """Carga o crea sistema V2"""
    try:
        from src.unified_system_v2 import UnifiedSystemV2
    except ImportError as e:
        logger.error(f"‚ùå No se pudo importar UnifiedSystemV2: {e}")
        logger.error("   Aseg√∫rate de que el archivo src/unified_system_v2.py existe")
        sys.exit(1)
    
    system_cache = Path("data/cache/unified_system_v2.pkl")
    
    if system_cache.exists():
        logger.info("üìÇ Cargando sistema V2 desde cache...")
        try:
            system = UnifiedSystemV2.load_from_cache()
            
            if system:
                logger.info(f"‚úÖ Sistema cargado: {len(system.canonical_products):,} productos")
                logger.info(f"   ‚Ä¢ RLHF: {'‚úÖ Disponible' if system.rl_ranker else '‚ùå No disponible'}")
                logger.info(f"   ‚Ä¢ NER: {'‚úÖ Disponible' if system.ner_ranker else '‚ùå No disponible'}")
                return system
            else:
                logger.warning("‚ö†Ô∏è  Sistema V2 no encontrado en cache, creando nuevo...")
        except Exception as e:
            logger.error(f"‚ùå Error cargando sistema desde cache: {e}")
            logger.warning("‚ö†Ô∏è  Creando nuevo sistema...")
    
    # Crear nuevo sistema
    logger.info("üî® Creando nuevo sistema V2...")
    system = UnifiedSystemV2()
    
    try:
        success = system.initialize_with_ner(
            limit=100000,  # L√≠mite para experimentos r√°pidos
            use_cache=True,
            use_zero_shot=False
        )
        
        if not success:
            logger.error("‚ùå Error inicializando sistema V2")
            sys.exit(1)
        
        return system
    except Exception as e:
        logger.error(f"‚ùå Error al inicializar sistema: {e}")
        traceback.print_exc()
        sys.exit(1)

def train_rlhf_on_system(system, train_queries: List[str]) -> bool:
    """Entrena RLHF en el sistema"""
    interactions_file = Path("data/interactions/real_interactions.jsonl")
    
    if not interactions_file.exists():
        logger.warning("‚ö†Ô∏è  No hay interacciones para entrenar RLHF")
        return False
    
    logger.info(f"üéØ Entrenando RLHF con {len(train_queries)} queries de entrenamiento...")
    
    try:
        success = system.train_rlhf_with_queries(train_queries, interactions_file)
        
        if success:
            logger.info("‚úÖ RLHF entrenado exitosamente")
            
            # Mostrar estad√≠sticas
            if hasattr(system, 'rl_ranker') and system.rl_ranker:
                stats = system.rl_ranker.get_stats()
                logger.info(f"   ‚Ä¢ Feedback procesado: {stats.get('feedback_count', 0)}")
                logger.info(f"   ‚Ä¢ Features aprendidas: {stats.get('weights_count', 0)}")
        else:
            logger.warning("‚ö†Ô∏è  RLHF no pudo ser entrenado (pocos datos?)")
        
        return success
    except Exception as e:
        logger.error(f"‚ùå Error entrenando RLHF: {e}")
        traceback.print_exc()
        return False

def calculate_ranking_metrics(ranked_ids: List[str], relevant_ids: List[str], 
                            k: int = 5) -> Dict[str, float]:
    """Calcula m√©tricas de ranking"""
    if not relevant_ids or k == 0:
        return {'mrr': 0.0, 'precision@k': 0.0, 'recall@k': 0.0, 'ndcg@k': 0.0}
    
    # MRR (Mean Reciprocal Rank)
    mrr = 0.0
    for i, pid in enumerate(ranked_ids):
        if pid in relevant_ids:
            mrr = 1.0 / (i + 1)
            break
    
    # Precision@k
    top_k = ranked_ids[:k]
    relevant_in_top = [pid for pid in top_k if pid in relevant_ids]
    precision = len(relevant_in_top) / k if k > 0 else 0.0
    
    # Recall@k
    recall = len(relevant_in_top) / len(relevant_ids)
    
    # NDCG@k
    dcg = 0.0
    for i, pid in enumerate(top_k):
        if pid in relevant_ids:
            dcg += 1.0 / np.log2(i + 2)
    
    # Ideal DCG
    ideal_relevance = [1] * min(len(relevant_ids), k)
    idcg = sum(1.0 / np.log2(i + 2) for i in range(len(ideal_relevance)))
    ndcg = dcg / idcg if idcg > 0 else 0.0
    
    return {
        'mrr': mrr,
        'precision@k': precision,
        'recall@k': recall,
        'ndcg@k': ndcg,
        'found': len(relevant_in_top)
    }

def evaluate_method_on_query(system, method: str, query: str, 
                           relevant_ids: List[str], k: int = 5) -> Dict[str, Any]:
    """Eval√∫a un m√©todo espec√≠fico en una query"""
    try:
        # Ejecutar m√©todo
        results = system.query_four_methods(query, k=k*2)
        method_results = results['methods'].get(method, [])
        
        if not method_results:
            return {
                'mrr': 0.0,
                'precision@k': 0.0,
                'recall@k': 0.0,
                'ndcg@k': 0.0,
                'found': 0,
                'success': False
            }
        
        # Convertir a IDs
        ranked_ids = []
        for product in method_results[:k]:
            product_id = getattr(product, 'id', None)
            if product_id:
                ranked_ids.append(product_id)
        
        # Calcular m√©tricas
        metrics = calculate_ranking_metrics(ranked_ids, relevant_ids, k)
        metrics['success'] = True
        
        return metrics
        
    except Exception as e:
        logger.error(f"‚ùå Error evaluando {method} en '{query}': {e}")
        return {
            'mrr': 0.0,
            'precision@k': 0.0,
            'recall@k': 0.0,
            'ndcg@k': 0.0,
            'found': 0,
            'success': False
        }

def run_statistical_analysis(results: Dict[str, List[Dict]]) -> Dict[str, Any]:
    """Ejecuta an√°lisis estad√≠stico"""
    try:
        from scipy import stats
        
        tests = {}
        baseline_key = 'baseline'
        
        if baseline_key not in results or len(results[baseline_key]) < 3:
            logger.warning("‚ö†Ô∏è  Insuficientes datos para an√°lisis estad√≠stico")
            return {}
        
        baseline_metrics = [r['mrr'] for r in results[baseline_key] if 'mrr' in r]
        
        for method in ['ner_enhanced', 'rlhf', 'full_hybrid']:
            if method not in results or len(results[method]) < 3:
                continue
            
            method_metrics = [r['mrr'] for r in results[method] if 'mrr' in r]
            
            if len(baseline_metrics) != len(method_metrics):
                logger.warning(f"‚ö†Ô∏è  N√∫mero de muestras diferente para {method}")
                continue
            
            if len(baseline_metrics) > 2:
                # Test t pareado
                t_stat, p_value = stats.ttest_rel(baseline_metrics, method_metrics)
                
                # Calcular mejoras
                baseline_mean = np.mean(baseline_metrics)
                method_mean = np.mean(method_metrics)
                mean_diff = method_mean - baseline_mean
                
                # Cohen's d
                pooled_std = np.sqrt((np.std(baseline_metrics)**2 + np.std(method_metrics)**2) / 2)
                cohens_d = mean_diff / pooled_std if pooled_std != 0 else 0
                
                # Porcentaje de mejora
                percent_improvement = (mean_diff / baseline_mean * 100) if baseline_mean > 0 else 0
                
                tests[method] = {
                    't_statistic': float(t_stat),
                    'p_value': float(p_value),
                    'significant': p_value < 0.05,
                    'cohens_d': float(cohens_d),
                    'mean_improvement': float(mean_diff),
                    'percent_improvement': float(percent_improvement),
                    'baseline_mean': float(baseline_mean),
                    'method_mean': float(method_mean),
                    'n_samples': len(baseline_metrics)
                }
        
        return tests
        
    except ImportError:
        logger.warning("‚ö†Ô∏è  SciPy no instalado. Skipping tests estad√≠sticos.")
        logger.info("   Instalar: pip install scipy")
        return {}
    except Exception as e:
        logger.error(f"‚ùå Error en an√°lisis estad√≠stico: {e}")
        return {}

class EnhancedJSONEncoder(json.JSONEncoder):
    """JSON encoder mejorado"""
    def default(self, obj):
        if isinstance(obj, np.float32):
            return float(obj)
        if isinstance(obj, np.float64):
            return float(obj)
        if isinstance(obj, np.int32):
            return int(obj)
        if isinstance(obj, np.int64):
            return int(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, (bool, np.bool_)):
            return bool(obj)
        if pd.isna(obj):
            return None
        return str(obj)

def save_results(results: Dict[str, Any], summary: Dict[str, Any], 
                tests: Dict[str, Any], train_queries: List[str], 
                test_queries: List[str]):
    """Guarda resultados del experimento"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # JSON con todos los datos
    json_file = Path(f"results/experimento_4_metodos_{timestamp}.json")
    
    # Preparar datos
    save_data = {
        'metadata': {
            'timestamp': timestamp,
            'train_queries_count': len(train_queries),
            'test_queries_count': len(test_queries),
            'split_ratio': '75/25',
            'seed': 42
        },
        'summary': {},
        'statistical_tests': {},
        'train_queries': train_queries,
        'test_queries': test_queries
    }
    
    # Convertir summary
    for method, stats in summary.items():
        save_data['summary'][method] = {}
        for key, value in stats.items():
            if isinstance(value, (np.float32, np.float64)):
                save_data['summary'][method][key] = float(value)
            elif isinstance(value, (np.int32, np.int64)):
                save_data['summary'][method][key] = int(value)
            elif pd.isna(value):
                save_data['summary'][method][key] = None
            else:
                save_data['summary'][method][key] = value
    
    # Convertir tests
    for method, test in tests.items():
        save_data['statistical_tests'][method] = {}
        for key, value in test.items():
            if isinstance(value, (np.float32, np.float64)):
                save_data['statistical_tests'][method][key] = float(value)
            elif isinstance(value, (np.int32, np.int64)):
                save_data['statistical_tests'][method][key] = int(value)
            elif isinstance(value, bool):
                save_data['statistical_tests'][method][key] = bool(value)
            elif pd.isna(value):
                save_data['statistical_tests'][method][key] = None
            else:
                save_data['statistical_tests'][method][key] = value
    
    # Guardar JSON
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, indent=2, ensure_ascii=False, cls=EnhancedJSONEncoder)
    
    logger.info(f"üíæ Resultados JSON: {json_file}")
    
    # CSV para an√°lisis
    csv_file = Path(f"results/experimento_4_metodos_{timestamp}.csv")
    
    rows = []
    methods = ['baseline', 'ner_enhanced', 'rlhf', 'full_hybrid']
    
    for method in methods:
        if method in results:
            for i, metrics in enumerate(results[method]):
                rows.append({
                    'method': method,
                    'query_idx': i,
                    'mrr': metrics.get('mrr', 0),
                    'precision@5': metrics.get('precision@k', 0),
                    'recall@5': metrics.get('recall@k', 0),
                    'ndcg@5': metrics.get('ndcg@k', 0),
                    'found': metrics.get('found', 0),
                    'success': bool(metrics.get('success', False))
                })
    
    if rows:
        df = pd.DataFrame(rows)
        df.to_csv(csv_file, index=False)
        logger.info(f"üìä Resultados CSV: {csv_file}")
    
    # Resumen en texto
    txt_file = Path(f"results/resumen_{timestamp}.txt")
    
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("RESUMEN DEL EXPERIMENTO - 4 M√âTODOS DE RANKING\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("üìä M√âTRICAS PROMEDIO (MRR):\n")
        f.write("-" * 50 + "\n")
        for method in methods:
            if method in summary:
                f.write(f"{method.replace('_', ' ').title():20} {summary[method]['mrr_mean']:.4f} ¬± {summary[method]['mrr_std']:.4f}\n")
        
        f.write("\nüìà TESTS ESTAD√çSTICOS (vs Baseline):\n")
        f.write("-" * 50 + "\n")
        for method in ['ner_enhanced', 'rlhf', 'full_hybrid']:
            if method in tests:
                sig = "‚úÖ SIGNIFICATIVO" if tests[method].get('significant', False) else "‚ö†Ô∏è  NO SIGNIFICATIVO"
                f.write(f"{method.replace('_', ' ').title():20} p={tests[method].get('p_value', 1.0):.4f} {sig}\n")
                f.write(f"                     Mejora: {tests[method].get('percent_improvement', 0.0):+.2f}% (d={tests[method].get('cohens_d', 0.0):.3f})\n")
        
        f.write("\n" + "=" * 80 + "\n")
        f.write("CONCLUSIONES\n")
        f.write("=" * 80 + "\n\n")
        
        # Determinar mejor m√©todo
        valid_methods = {k: v for k, v in summary.items() if v.get('mrr_mean', 0) > 0}
        if valid_methods:
            best_method = max(valid_methods.items(), key=lambda x: x[1]['mrr_mean'])[0]
            
            if 'baseline' in summary and summary['baseline']['mrr_mean'] > 0:
                baseline_mrr = summary['baseline']['mrr_mean']
                best_mrr = summary[best_method]['mrr_mean']
                improvement = ((best_mrr / baseline_mrr) - 1) * 100
                
                f.write(f"üèÜ MEJOR M√âTODO: {best_method.replace('_', ' ').title()}\n")
                f.write(f"   ‚Ä¢ MRR Baseline: {baseline_mrr:.4f}\n")
                f.write(f"   ‚Ä¢ MRR Mejor:    {best_mrr:.4f}\n")
                f.write(f"   ‚Ä¢ Mejora:       {improvement:+.2f}%\n\n")
                
                is_significant = best_method in tests and tests[best_method].get('significant', False)
                
                if improvement > 5 and is_significant:
                    f.write("‚úÖ ¬°MEJORA SIGNIFICATIVA Y RELEVANTE!\n")
                    f.write("   El sistema funciona correctamente.\n")
                elif improvement > 0:
                    f.write("‚ö†Ô∏è  MEJORA PEQUE√ëA\n")
                    f.write("   Considera recolectar m√°s datos.\n")
                else:
                    f.write("‚ùå SIN MEJORA\n")
                    f.write("   Revisa la implementaci√≥n.\n")
            else:
                f.write(f"üèÜ MEJOR M√âTODO: {best_method.replace('_', ' ').title()}\n")
                f.write(f"   ‚Ä¢ MRR: {summary[best_method]['mrr_mean']:.4f}\n")
                f.write("   ‚Ä¢ Baseline no funcion√≥ (MRR=0)\n")
        else:
            f.write("‚ùå NING√öN M√âTODO FUNCION√ì CORRECTAMENTE\n")
            f.write("   ‚Ä¢ Todos los m√©todos tuvieron MRR=0\n")
            f.write("   ‚Ä¢ Verifica los errores en los logs\n")
    
    logger.info(f"üìù Resumen: {txt_file}")
    
    return json_file, csv_file, txt_file

# ============================================================================
# FUNCI√ìN PRINCIPAL
# ============================================================================

def main():
    """Funci√≥n principal del experimento"""
    try:
        print("\n" + "="*80)
        print("üî¨ EXPERIMENTO COMPLETO: 4 M√âTODOS DE RANKING")
        print("="*80)
        
        # 1. Cargar ground truth
        logger.info("üìÇ Cargando ground truth...")
        ground_truth = load_ground_truth()
        
        # 2. Split train/test
        logger.info("‚úÇÔ∏è  Creando split train/test estratificado...")
        train_queries, test_queries = split_train_test_stratified(
            ground_truth, test_size=0.25, seed=42
        )
        
        # Limitar test queries para velocidad
        max_test_queries = min(30, len(test_queries))
        test_queries = test_queries[:max_test_queries]
        
        print(f"\nüìä DATASET:")
        print(f"   ‚Ä¢ Total queries: {len(ground_truth)}")
        print(f"   ‚Ä¢ Train queries: {len(train_queries)}")
        print(f"   ‚Ä¢ Test queries: {len(test_queries)}")
        
        # 3. Cargar sistema
        logger.info("ü§ñ Cargando sistema V2...")
        system = load_or_create_system_v2()
        
        print(f"\nü§ñ SISTEMA:")
        print(f"   ‚Ä¢ Productos: {len(system.canonical_products):,}")
        print(f"   ‚Ä¢ RLHF: {'‚úÖ Disponible' if hasattr(system, 'rl_ranker') and system.rl_ranker else '‚ùå No entrenado'}")
        print(f"   ‚Ä¢ NER: {'‚úÖ Disponible' if hasattr(system, 'ner_ranker') and system.ner_ranker else '‚ùå No disponible'}")
        
        # 4. Entrenar RLHF
        train_rlhf_on_system(system, train_queries)
        
        # 5. Evaluar m√©todos
        print(f"\nüß™ EVALUANDO 4 M√âTODOS...")
        
        methods = ['baseline', 'ner_enhanced', 'rlhf', 'full_hybrid']
        results = {method: [] for method in methods}
        
        for i, query in enumerate(test_queries, 1):
            relevant_ids = ground_truth.get(query, [])
            
            if not relevant_ids:
                continue
            
            if i % 5 == 0 or i == 1 or i == len(test_queries):
                print(f"   [{i}/{len(test_queries)}] '{query[:40]}...'")
            
            for method in methods:
                metrics = evaluate_method_on_query(system, method, query, relevant_ids, k=5)
                results[method].append(metrics)
        
        # 6. Calcular resumen
        print(f"\nüìä CALCULANDO RESULTADOS...")
        
        summary = {}
        for method in methods:
            if method in results and results[method]:
                method_results = [r for r in results[method] if r.get('success', False)]
                
                if method_results:
                    df = pd.DataFrame(method_results)
                    summary[method] = {
                        'mrr_mean': float(df['mrr'].mean()),
                        'mrr_std': float(df['mrr'].std()),
                        'precision_mean': float(df['precision@k'].mean()),
                        'recall_mean': float(df['recall@k'].mean()),
                        'ndcg_mean': float(df['ndcg@k'].mean()),
                        'total_found': int(df['found'].sum()),
                        'n_queries': len(method_results),
                        'success_rate': float(len(method_results) / len(results[method]))
                    }
                else:
                    summary[method] = {
                        'mrr_mean': 0.0,
                        'mrr_std': 0.0,
                        'precision_mean': 0.0,
                        'recall_mean': 0.0,
                        'ndcg_mean': 0.0,
                        'total_found': 0,
                        'n_queries': 0,
                        'success_rate': 0.0
                    }
        
        # 7. An√°lisis estad√≠stico
        tests = run_statistical_analysis(results)
        
        # 8. Mostrar resultados
        print("\n" + "="*80)
        print("üìà RESULTADOS FINALES")
        print("="*80)
        
        print(f"\n{'M√©todo':<20} {'MRR':<8} {'P@5':<8} {'R@5':<8} {'NDCG@5':<8} {'Found':<8}")
        print("-" * 70)
        
        for method in methods:
            if method in summary:
                stats = summary[method]
                print(f"{method.replace('_', ' ').title():<20} "
                      f"{stats['mrr_mean']:.4f}  "
                      f"{stats['precision_mean']:.4f}  "
                      f"{stats['recall_mean']:.4f}  "
                      f"{stats['ndcg_mean']:.4f}  "
                      f"{stats['total_found']:>6}")
        
        if tests:
            print(f"\nüìä SIGNIFICANCIA ESTAD√çSTICA (vs Baseline)")
            print("-" * 50)
            
            for method in ['ner_enhanced', 'rlhf', 'full_hybrid']:
                if method in tests:
                    test = tests[method]
                    sig = "‚úÖ" if test.get('significant', False) else "‚ö†Ô∏è "
                    print(f"{method.replace('_', ' ').title():20} "
                          f"p={test.get('p_value', 1.0):.4f} {sig} "
                          f"Mejora: {test.get('percent_improvement', 0.0):+.2f}%")
        
        # 9. Guardar resultados
        logger.info("üíæ Guardando resultados...")
        json_file, csv_file, txt_file = save_results(results, summary, tests, 
                                                    train_queries, test_queries)
        
        # 10. Conclusiones
        print("\n" + "="*80)
        print("üí° CONCLUSIONES Y RECOMENDACIONES")
        print("="*80)
        
        if 'full_hybrid' in summary and summary['full_hybrid']['mrr_mean'] > 0:
            if 'baseline' in summary and summary['baseline']['mrr_mean'] > 0:
                baseline_mrr = summary['baseline']['mrr_mean']
                hybrid_mrr = summary['full_hybrid']['mrr_mean']
                improvement = ((hybrid_mrr / baseline_mrr) - 1) * 100
                
                print(f"\nüèÜ FULL HYBRID vs BASELINE:")
                print(f"   ‚Ä¢ Baseline MRR:  {baseline_mrr:.4f}")
                print(f"   ‚Ä¢ Hybrid MRR:    {hybrid_mrr:.4f}")
                print(f"   ‚Ä¢ Mejora:        {improvement:+.2f}%")
                
                is_significant = 'full_hybrid' in tests and tests['full_hybrid'].get('significant', False)
                
                if improvement > 5 and is_significant:
                    print(f"\n‚úÖ ¬°EXCELENTE! MEJORA SIGNIFICATIVA (>5%)")
                    print(f"   ‚Ä¢ Tu sistema funciona correctamente")
                    print(f"   ‚Ä¢ RLHF y NER est√°n aportando valor")
                    print(f"   ‚Ä¢ Puedes proceder con paper IEEE")
                elif improvement > 0:
                    print(f"\n‚ö†Ô∏è  MEJORA PEQUE√ëA ({improvement:+.2f}%)")
                    print(f"   ‚Ä¢ Recomendado: Recolectar m√°s datos")
                else:
                    print(f"\n‚ùå SIN MEJORA ({improvement:+.2f}%)")
                    print(f"   ‚Ä¢ Posibles causas:")
                    print(f"     1. Baseline demasiado bueno")
                    print(f"     2. Insuficientes datos de entrenamiento")
        
        print(f"\nüìã PR√ìXIMOS PASOS:")
        print(f"1. Revisar resultados: {txt_file}")
        if 'full_hybrid' in summary and summary['full_hybrid']['mrr_mean'] > 0.1:
            print(f"2. Si MRR > 0.1: ¬°Prepara paper!")
        else:
            print(f"2. Si MRR < 0.1: Recolectar m√°s feedback")
        print(f"3. Ejecutar: python main.py interactivo (m√°s clicks)")
        
        print(f"\n‚úÖ EXPERIMENTO COMPLETADO")
        print(f"   ‚Ä¢ Archivos guardados en: results/")
        
        logger.info("üéâ Experimento completado exitosamente")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Experimento interrumpido por el usuario")
        logger.warning("Experimento interrumpido por el usuario")
    except Exception as e:
        logger.error(f"‚ùå Error en experimento: {e}")
        traceback.print_exc()
        sys.exit(1)

# ============================================================================
# EJECUCI√ìN PRINCIPAL
# ============================================================================

if __name__ == "__main__":
    main()