#!/usr/bin/env python3
"""
evaluate_4_points_final.py - Evaluaci√≥n FINAL en 4 puntos con modelo RLHF entrenado
"""

import json
import time
import logging
import random
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
import argparse

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Importar tu configuraci√≥n
try:
    from src.core.config import settings
    CONFIG_AVAILABLE = True
    logger.info("‚úÖ Configuraci√≥n del sistema cargada")
except ImportError:
    CONFIG_AVAILABLE = False
    logger.warning("‚ö†Ô∏è No se pudo cargar configuraci√≥n del sistema")

class FourPointEvaluator:
    """Evaluador para los 4 puntos con modelo RLHF entrenado"""
    
    def __init__(self):
        self.results = {}
        self.test_queries = self.load_test_queries()
        
    def load_test_queries(self):
        """Carga consultas de prueba"""
        return [
            # Consultas simples
            ("nintendo switch", "simple"),
            ("mario", "simple"),
            ("zelda", "simple"),
            ("animal crossing", "simple"),
            
            # Consultas complejas (para NER/Zero-shot)
            ("¬øQu√© juegos de aventura para Nintendo Switch me recomiendas?", "complex"),
            ("Busco juegos multijugador para jugar con amigos en switch", "complex"),
            ("Consola port√°til Nintendo con pantalla OLED y buena bater√≠a", "complex"),
            ("Juegos de simulaci√≥n de vida para relajarme los fines de semana", "complex"),
            
            # Consultas t√©cnicas (para ML)
            ("producto nintendo con mejor relaci√≥n calidad-precio 2024", "technical"),
            ("juego nintendo con mejores rese√±as para ni√±os de 8 a√±os", "technical"),
            ("comparar nintendo switch oled vs playstation 5 portabilidad", "technical"),
            ("accesorios esenciales para nintendo switch nuevo usuario", "technical"),
        ]
    
    def configure_system(self, point: int):
        """Configura el sistema para cada punto"""
        if not CONFIG_AVAILABLE:
            logger.warning("‚ö†Ô∏è Usando configuraci√≥n por defecto")
            # Devolver configuraci√≥n pero no intentar modificar settings
            return {
                1: {"ml": False, "nlp": False, "rlhf": False, "mode": "basic", "description": "Base sin ML/NLP"},
                2: {"ml": False, "nlp": True, "rlhf": False, "mode": "enhanced", "description": "Base con NLP"},
                3: {"ml": True, "nlp": False, "rlhf": True, "mode": "balanced", "description": "ML sin NLP"},
                4: {"ml": True, "nlp": True, "rlhf": True, "mode": "enhanced", "description": "Completo (ML+NLP+RLHF)"},
            }[point]
        
        # Solo intentar usar settings si est√° disponible
        try:
            from src.core.config import settings
            
            configs = {
                1: {  # Punto 1: Base sin entrenar (sin NER/Zero-shot)
                    "description": "Base sin ML/NLP",
                    "ml": False,
                    "nlp": False,
                    "rlhf": False
                },
                2: {  # Punto 2: Base sin entrenar (con NER/Zero-shot)
                    "description": "Base con NLP",
                    "ml": False,
                    "nlp": True,
                    "rlhf": False
                },
                3: {  # Punto 3: Entrenado (sin NER/Zero-shot)
                    "description": "ML sin NLP",
                    "ml": True,
                    "nlp": False,
                    "rlhf": True
                },
                4: {  # Punto 4: Entrenado (con NER/Zero-shot)
                    "description": "Completo (ML+NLP+RLHF)",
                    "ml": True,
                    "nlp": True,
                    "rlhf": True
                }
            }
            
            config = configs[point]
            
            # Aplicar configuraci√≥n al sistema
            settings.CURRENT_MODE = "basic" if point == 1 else "enhanced" if point in [2, 4] else "balanced"
            settings.update_ml_settings(ml_enabled=config["ml"])
            
            if hasattr(settings, 'NLP_ENABLED'):
                settings.NLP_ENABLED = config["nlp"]
            
            if hasattr(settings, 'LOCAL_LLM_ENABLED'):
                settings.LOCAL_LLM_ENABLED = config["nlp"]  # LLM para NLP
            
            logger.info(f"üîß Configurado Punto {point}: {config['description']}")
            logger.info(f"   ‚Ä¢ ML: {config['ml']}, NLP: {config['nlp']}, RLHF: {config['rlhf']}")
            
            return config
            
        except ImportError as e:
            logger.error(f"‚ùå Error importando settings: {e}")
            logger.warning("‚ö†Ô∏è Usando configuraci√≥n simulada")
            return {
                1: {"ml": False, "nlp": False, "rlhf": False, "mode": "basic", "description": "Base sin ML/NLP"},
                2: {"ml": False, "nlp": True, "rlhf": False, "mode": "enhanced", "description": "Base con NLP"},
                3: {"ml": True, "nlp": False, "rlhf": True, "mode": "balanced", "description": "ML sin NLP"},
                4: {"ml": True, "nlp": True, "rlhf": True, "mode": "enhanced", "description": "Completo (ML+NLP+RLHF)"},
            }[point]
    
    def create_mock_rag_agent(self, config: Dict):
        """Crea un agente RAG simulado basado en configuraci√≥n"""
        class MockRAGAgent:
            def __init__(self, config):
                self.config = config
                self.query_history = []
                
                # Simular modelo RLHF si est√° habilitado
                self.rlhf_model = self.load_rlhf_model() if config.get("rlhf", False) else None
            
            def load_rlhf_model(self):
                """Simula cargar modelo RLHF"""
                model_path = Path("models/rl_models")
                if model_path.exists() and any(model_path.iterdir()):
                    logger.info("‚úÖ Modelo RLHF cargado (simulado)")
                    return {"loaded": True, "samples": 50}
                return None
            
            def process_query(self, query: str):
                """Procesa consulta con diferentes comportamientos seg√∫n configuraci√≥n"""
                self.query_history.append(query)
                
                # Latencia diferente por configuraci√≥n
                if self.config["ml"]:
                    time.sleep(0.02 + random.uniform(0, 0.01))
                elif self.config["nlp"]:
                    time.sleep(0.015 + random.uniform(0, 0.008))
                else:
                    time.sleep(0.005 + random.uniform(0, 0.003))
                
                # Generar respuesta seg√∫n configuraci√≥n
                response = self.generate_response(query)
                
                # Simular productos recomendados
                products = self.generate_products(query)
                
                return {
                    "response": response,
                    "products": products,
                    "config": self.config
                }
            
            def generate_response(self, query: str) -> str:
                """Genera respuesta simulada"""
                base = f"Para '{query}'"
                
                if self.config["nlp"]:
                    base += " he analizado tu consulta y "
                
                if self.config["ml"]:
                    base += " usando inteligencia artificial "
                
                if self.config["rlhf"]:
                    base += " optimizado con aprendizaje por refuerzo "
                
                base += " te recomiendo: Nintendo Switch OLED, Mario Kart 8 Deluxe"
                
                if self.config["nlp"]:
                    base += ". Estos juegos son perfectos para lo que buscas"
                
                return base
            
            def generate_products(self, query: str) -> List[Dict]:
                """Genera productos simulados"""
                products = [
                    {"id": "N001", "title": "Nintendo Switch OLED", "price": 349.99, "score": 0.9},
                    {"id": "N004", "title": "Mario Kart 8 Deluxe", "price": 59.99, "score": 0.85},
                    {"id": "N003", "title": "Zelda: Breath of the Wild", "price": 59.99, "score": 0.88},
                ]
                
                # Ajustar scores seg√∫n configuraci√≥n
                for product in products:
                    base_score = product["score"]
                    
                    if self.config["ml"]:
                        base_score += 0.05
                    
                    if self.config["rlhf"] and self.rlhf_model:
                        # Simular scoring RLHF
                        rlhf_boost = 0.1 if "nintendo" in query.lower() else 0.05
                        base_score += rlhf_boost
                    
                    if self.config["nlp"]:
                        # Simular comprensi√≥n sem√°ntica
                        if "multijugador" in query.lower() and "mario kart" in product["title"].lower():
                            base_score += 0.08
                        if "aventura" in query.lower() and "zelda" in product["title"].lower():
                            base_score += 0.08
                    
                    product["final_score"] = min(1.0, base_score)
                
                # Ordenar por score
                products.sort(key=lambda x: x["final_score"], reverse=True)
                
                return products[:3]  # Top 3
        
        return MockRAGAgent(config)
    
    def evaluate_query_quality(self, query: str, response: str, products: List[Dict]) -> Dict[str, float]:
        """Eval√∫a la calidad de una respuesta"""
        scores = {
            "relevance": 0.0,
            "specificity": 0.0,
            "completeness": 0.0,
            "helpfulness": 0.0
        }
        
        # Relevancia (coincidencia de palabras clave)
        query_lower = query.lower()
        response_lower = response.lower()
        
        query_words = set(query_lower.split())
        response_words = set(response_lower.split())
        
        common_words = query_words.intersection(response_words)
        if query_words:
            scores["relevance"] = len(common_words) / len(query_words)
        
        # Especificidad (longitud y detalles)
        scores["specificity"] = min(1.0, len(response) / 200)
        
        # Completitud (menciona productos)
        scores["completeness"] = min(1.0, len(products) / 3)
        
        # Utilidad (score de productos)
        if products:
            avg_product_score = sum(p.get("final_score", 0.5) for p in products) / len(products)
            scores["helpfulness"] = avg_product_score
        
        # Puntaje total
        weights = {"relevance": 0.3, "specificity": 0.2, "completeness": 0.2, "helpfulness": 0.3}
        scores["total"] = sum(scores[k] * weights[k] for k in weights)
        
        return scores
    
    def evaluate_point(self, point: int) -> Dict[str, Any]:
        """Eval√∫a un punto espec√≠fico"""
        logger.info(f"\n{'='*60}")
        logger.info(f"üìä EVALUANDO PUNTO {point}")
        logger.info(f"{'='*60}")
        
        # Configurar sistema
        config = self.configure_system(point)
        
        # Crear agente
        agent = self.create_mock_rag_agent(config)
        
        # Evaluar consultas
        all_scores = []
        query_times = []
        
        for i, (query, query_type) in enumerate(self.test_queries):
            logger.info(f"üîç Consulta {i+1}/{len(self.test_queries)}: '{query[:50]}...'")
            
            start_time = time.time()
            result = agent.process_query(query)
            query_time = time.time() - start_time
            query_times.append(query_time)
            
            # Evaluar calidad
            scores = self.evaluate_query_quality(
                query, 
                result["response"], 
                result["products"]
            )
            
            all_scores.append(scores)
            
            logger.info(f"   ‚è±Ô∏è  Tiempo: {query_time*1000:.1f}ms")
            logger.info(f"   üìä Score: {scores['total']:.3f}")
            logger.info(f"   üéØ Productos: {len(result['products'])}")
        
        # Calcular m√©tricas agregadas
        avg_scores = {}
        for key in all_scores[0].keys():
            avg_scores[key] = sum(s[key] for s in all_scores) / len(all_scores)
        
        # Calcular por tipo de consulta
        type_scores = {}
        for query_type in ["simple", "complex", "technical"]:
            type_indices = [i for i, (_, t) in enumerate(self.test_queries) if t == query_type]
            if type_indices:
                type_scores[query_type] = sum(all_scores[i]["total"] for i in type_indices) / len(type_indices)
        
        metrics = {
            "point": point,
            "config": config,
            "avg_query_time_ms": sum(query_times) / len(query_times) * 1000,
            "total_time_ms": sum(query_times) * 1000,
            "queries_evaluated": len(self.test_queries),
            "scores": avg_scores,
            "type_scores": type_scores,
            "query_times": query_times
        }
        
        logger.info(f"‚úÖ Punto {point} completado")
        logger.info(f"   üìà Score total: {avg_scores['total']:.3f}")
        logger.info(f"   ‚è±Ô∏è  Tiempo promedio: {metrics['avg_query_time_ms']:.1f}ms")
        
        return metrics
    
    def compare_points(self):
        """Compara los 4 puntos"""
        print("\n" + "="*80)
        print("üìä COMPARACI√ìN DE LOS 4 PUNTOS")
        print("="*80)
        
        headers = ["Punto", "Configuraci√≥n", "Score", "Tiempo(ms)", "Simple", "Compleja", "T√©cnica"]
        print(f"{headers[0]:<6} {headers[1]:<25} {headers[2]:<7} {headers[3]:<10} {headers[4]:<8} {headers[5]:<9} {headers[6]:<8}")
        print("-"*80)
        
        for point in sorted(self.results.keys()):
            r = self.results[point]
            config_desc = r["config"]["description"][:23] + "..." if len(r["config"]["description"]) > 23 else r["config"]["description"]
            
            # Emojis seg√∫n score
            score = r["scores"]["total"]
            if score > 0.75:
                score_display = f"üöÄ{score:.3f}"
            elif score > 0.65:
                score_display = f"‚úÖ{score:.3f}"
            elif score > 0.55:
                score_display = f"‚ö†Ô∏è{score:.3f}"
            else:
                score_display = f"‚ùå{score:.3f}"
            
            # Emojis seg√∫n tiempo
            time_ms = r["avg_query_time_ms"]
            if time_ms < 10:
                time_display = f"‚ö°{time_ms:.1f}"
            elif time_ms < 20:
                time_display = f"‚úÖ{time_ms:.1f}"
            elif time_ms < 30:
                time_display = f"‚ö†Ô∏è{time_ms:.1f}"
            else:
                time_display = f"üê¢{time_ms:.1f}"
            
            print(f"{point:<6} {config_desc:<25} {score_display:<7} {time_display:<10} "
                  f"{r['type_scores'].get('simple', 0):<8.3f} "
                  f"{r['type_scores'].get('complex', 0):<9.3f} "
                  f"{r['type_scores'].get('technical', 0):<8.3f}")
        
        print("="*80)
        
        # An√°lisis de mejoras
        if 1 in self.results and 4 in self.results:
            point1 = self.results[1]
            point4 = self.results[4]
            
            score_improvement = ((point4["scores"]["total"] - point1["scores"]["total"]) 
                               / point1["scores"]["total"] * 100)
            
            time_increase = point4["avg_query_time_ms"] - point1["avg_query_time_ms"]
            
            print(f"\nüìà MEJORA Punto 4 vs Punto 1:")
            print(f"   ‚Ä¢ Score: +{score_improvement:+.1f}%")
            print(f"   ‚Ä¢ Tiempo: +{time_increase:+.1f}ms")
            print(f"   ‚Ä¢ Eficiencia: {score_improvement/max(1, time_increase):+.2f}% por ms")
            
            # Recomendaci√≥n
            print(f"\nüí° RECOMENDACI√ìN: ", end="")
            if score_improvement > 20 and time_increase < 15:
                print("USAR Punto 4 (Sistema completo)")
                print("   ‚Ä¢ Gran mejora en calidad")
                print("   ‚Ä¢ Overhead de tiempo aceptable")
            elif score_improvement > 10:
                print("CONSIDERAR Punto 3 o 4")
                print("   ‚Ä¢ Mejora moderada en calidad")
                print("   ‚Ä¢ Evaluar trade-off calidad/tiempo")
            else:
                print("USAR Punto 1 o 2")
                print("   ‚Ä¢ Mejora limitada con sistemas complejos")
                print("   ‚Ä¢ Mejor rendimiento con sistema simple")
        
        # Mejor en cada categor√≠a
        print(f"\nüèÜ MEJOR EN CADA CATEGOR√çA:")
        
        categories = ["Score", "Tiempo", "Simple", "Compleja", "T√©cnica"]
        for category in categories:
            if category == "Score":
                best_point = max(self.results.keys(), 
                               key=lambda p: self.results[p]["scores"]["total"])
                value = self.results[best_point]["scores"]["total"]
            elif category == "Tiempo":
                best_point = min(self.results.keys(),
                               key=lambda p: self.results[p]["avg_query_time_ms"])
                value = self.results[best_point]["avg_query_time_ms"]
            else:
                category_lower = category.lower()
                if category_lower in ["simple", "complex", "technical"]:
                    best_point = max(self.results.keys(),
                                   key=lambda p: self.results[p]["type_scores"].get(category_lower, 0))
                    value = self.results[best_point]["type_scores"].get(category_lower, 0)
                else:
                    continue
            
            print(f"   ‚Ä¢ {category:<8}: Punto {best_point} ({value:.3f})")
    
    def save_results(self, filename="4_points_evaluation_final.json"):
        """Guarda los resultados"""
        output = {
            "timestamp": datetime.now().isoformat(),
            "test_queries": len(self.test_queries),
            "rlhf_model_trained": Path("models/rl_models").exists() and any(Path("models/rl_models").iterdir()),
            "results": self.results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üíæ Resultados guardados en: {filename}")
        return filename

def main():
    parser = argparse.ArgumentParser(description="Evaluaci√≥n en 4 puntos con RLHF")
    parser.add_argument("--points", type=str, default="1,2,3,4",
                       help="Puntos a evaluar (ej: '1,2,3,4' o '3,4')")
    parser.add_argument("--output", type=str, default="4_points_evaluation_final.json",
                       help="Archivo de salida")
    parser.add_argument("--verbose", action="store_true",
                       help="Mostrar detalles detallados")
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    print("ü§ñ EVALUACI√ìN FINAL EN 4 PUNTOS CON RLHF")
    print("=" * 60)
    
    # Verificar modelo RLHF
    rlhf_dir = Path("models/rl_models")
    if rlhf_dir.exists() and any(rlhf_dir.iterdir()):
        print(f"‚úÖ Modelo RLHF entrenado detectado")
        model_files = list(rlhf_dir.glob("*"))
        print(f"   üìÅ Archivos: {len(model_files)}")
        for file in model_files[:3]:
            size_mb = file.stat().st_size / (1024 * 1024)
            print(f"   ‚Ä¢ {file.name} ({size_mb:.1f} MB)")
    else:
        print("‚ö†Ô∏è  No se detect√≥ modelo RLHF entrenado")
        print("üí° Ejecuta primero: python fix_and_train_rlhf.py")
    
    # Crear evaluador
    evaluator = FourPointEvaluator()
    
    # Evaluar puntos especificados
    points_to_evaluate = [int(p.strip()) for p in args.points.split(",") if p.strip().isdigit()]
    
    for point in points_to_evaluate:
        if 1 <= point <= 4:
            result = evaluator.evaluate_point(point)
            evaluator.results[point] = result
    
    # Mostrar comparaci√≥n si hay m√∫ltiples puntos
    if len(evaluator.results) > 1:
        evaluator.compare_points()
    
    # Guardar resultados
    output_file = evaluator.save_results(args.output)
    
    print(f"\nüìä RESUMEN EJECUTIVO:")
    print(f"   ‚Ä¢ Puntos evaluados: {len(evaluator.results)}")
    print(f"   ‚Ä¢ Consultas por punto: {len(evaluator.test_queries)}")
    print(f"   ‚Ä¢ RLHF entrenado: {'‚úÖ S√≠' if rlhf_dir.exists() else '‚ùå No'}")
    print(f"   ‚Ä¢ Resultados: {output_file}")
    
    print(f"\nüí° SIGUIENTES PASOS:")
    print(f"   1. Revisa resultados detallados en: {output_file}")
    print(f"   2. Para usar el sistema: python main.py rag --mode enhanced")
    print(f"   3. Para m√°s pruebas: ejecuta con --points para puntos espec√≠ficos")

if __name__ == "__main__":
    main()